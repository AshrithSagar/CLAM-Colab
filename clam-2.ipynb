{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Create Patches","metadata":{}},{"cell_type":"code","source":"source_dir = os.path.abspath(\"/kaggle/input/fungal-10x/\")\npatch_dir = os.path.abspath('/kaggle/working/patches/')\npatch_size = 256","metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:47:07.497428Z","iopub.execute_input":"2022-12-29T12:47:07.497808Z","iopub.status.idle":"2022-12-29T12:47:07.502549Z","shell.execute_reply.started":"2022-12-29T12:47:07.497777Z","shell.execute_reply":"2022-12-29T12:47:07.501698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nfrom itertools import product\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:44:59.629224Z","iopub.execute_input":"2022-12-29T12:44:59.629711Z","iopub.status.idle":"2022-12-29T12:44:59.637306Z","shell.execute_reply.started":"2022-12-29T12:44:59.629660Z","shell.execute_reply":"2022-12-29T12:44:59.635185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input/fungal-10x'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:45:17.953092Z","iopub.execute_input":"2022-12-29T12:45:17.954130Z","iopub.status.idle":"2022-12-29T12:45:17.964051Z","shell.execute_reply.started":"2022-12-29T12:45:17.954075Z","shell.execute_reply":"2022-12-29T12:45:17.963124Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tile(filename, dir_in, dir_out, d):\n    if not os.path.isdir(dir_out):\n        os.mkdir(dir_out)\n\n    name, ext = os.path.splitext(filename)\n    img = Image.open(os.path.join(dir_in, filename))\n    w, h = img.size\n\n    grid = product(range(0, h-h%d, d), range(0, w-w%d, d))\n    for i, j in grid:\n        box = (j, i, j+d, i+d)\n        i /= 256\n        j /= 256\n        out = os.path.join(dir_out, f'{name}_{int(i)}_{int(j)}{ext}')\n        img.crop(box).save(out)\n\n\nif not os.path.isdir(patch_dir):\n    os.mkdir(patch_dir)\n\nfor filename in os.listdir(source_dir):\n    name, ext = os.path.splitext(filename)\n    output_patches_dir = os.path.join(patch_dir, name)\n    \n    print(\"Patching\", filename)\n    tile(filename, source_dir, output_patches_dir, patch_size)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:47:36.184540Z","iopub.execute_input":"2022-12-29T12:47:36.185631Z","iopub.status.idle":"2022-12-29T12:48:04.581573Z","shell.execute_reply.started":"2022-12-29T12:47:36.185591Z","shell.execute_reply":"2022-12-29T12:48:04.580424Z"},"scrolled":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{"_kg_hide-output":false}},{"cell_type":"code","source":"patch_dir = os.path.abspath('/kaggle/working/patches/')\nfeat_dir = os.path.abspath('/kaggle/working/features/')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T13:12:08.984488Z","iopub.execute_input":"2022-12-29T13:12:08.984953Z","iopub.status.idle":"2022-12-29T13:12:08.993935Z","shell.execute_reply.started":"2022-12-29T13:12:08.984916Z","shell.execute_reply":"2022-12-29T13:12:08.993002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport yaml\nimport argparse\n\n# import h5py\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.np_utils import to_categorical\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n\nfrom tensorflow.keras.utils import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom keras.callbacks import ModelCheckpoint\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-12-29T13:12:08.974343Z","iopub.execute_input":"2022-12-29T13:12:08.975545Z","iopub.status.idle":"2022-12-29T13:12:08.982667Z","shell.execute_reply.started":"2022-12-29T13:12:08.975502Z","shell.execute_reply":"2022-12-29T13:12:08.981627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create feat_dir if not exists.\nif not os.path.isdir(feat_dir):\n    os.mkdir(feat_dir)\n\n# Loading ResNet50 wit imagenet weights, include_top means that we loading model without last fully connected layers\nmodel = ResNet50(weights = 'imagenet', include_top = False)\n\n# patch_folders = [os.path.join(patch_dir, folder) for folder in sorted(os.listdir(patch_dir))]\n# patches_per_image = len(os.listdir(patch_folders[0]))\n# print(patches_per_image)\n\n# Create dataset from the image patches\nfor folder in sorted(os.listdir(patch_dir)):\n    filename = str(folder).split(\"/\")[-1]\n    filePath = os.path.join(feat_dir, filename)\n    # Run only if file doesn't already exist\n    if os.path.exists(filePath):\n        print(\"Skipping File:\", filename)\n        continue\n    print(\"Running on File:\", filename)\n\n    features = []\n    patch_folder = os.path.join(patch_dir, folder)\n    for patch_file in sorted(os.listdir(patch_folder)):\n        img_path = os.path.join(patch_folder, patch_file)\n\n        # Get coord in [x, y] format\n        coord = img_path.split(\"/\")\n        coord = coord[-1]\n        coord = coord.split(\".\")[-2]\n        coord = coord.split(\"_\")\n        coord = [int(coord[-2])/256, int(coord[-1])/256]\n\n        # Read image\n        orig = cv2.imread(img_path)\n\n        # Convert image to RGB from BGR (another way is to use \"image = image[:, :, ::-1]\" code)\n        orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n\n        # Resize image to 224x224 size\n        image = cv2.resize(orig, (224, 224)).reshape(-1, 224, 224, 3)\n\n        # We need to preprocess imageto fulfill ResNet50 requirements\n        image = preprocess_input(image)\n\n        # Extracting our features\n        feature = model.predict(image)\n\n        # Group the features\n        features.append(feature)\n    np.save(filePath, features)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T13:14:01.753992Z","iopub.execute_input":"2022-12-29T13:14:01.754395Z","iopub.status.idle":"2022-12-29T13:21:33.976760Z","shell.execute_reply.started":"2022-12-29T13:14:01.754364Z","shell.execute_reply":"2022-12-29T13:21:33.975804Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset creator","metadata":{}},{"cell_type":"code","source":"filename = os.path.join('/kaggle/working/', 'fungal_vs_nonfungal.csv')\npatch_dir = os.path.abspath('/kaggle/working/patches/')\nfeat_dir = os.path.abspath('/kaggle/working/features/')\nannotated_dir = os.path.abspath('/kaggle/input/fungal-10x-annot/')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T13:21:45.262774Z","iopub.execute_input":"2022-12-29T13:21:45.263667Z","iopub.status.idle":"2022-12-29T13:21:45.269362Z","shell.execute_reply.started":"2022-12-29T13:21:45.263618Z","shell.execute_reply":"2022-12-29T13:21:45.268164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(filename, 'w') as file:\n    file.write('case_id,slide_id,label' + '\\n')\n\n    patch_folders = [os.path.join(patch_dir, folder) for folder in sorted(os.listdir(patch_dir))]\n\n    for i, name in enumerate(patch_folders):\n        name = name.split(\"/\")[-1]\n        if name != feat_dir:\n            if name[0] == \"F\":\n                f_nf = \"fungal\"\n            elif name[0] == \"N\":\n                f_nf = \"nonfungal\"\n                annotated = True\n            else:\n                f_nf = \"unclassified\"\n\n            line = 'case_' + str(i) + ',' + name + ',' + f_nf\n            file.write('{}\\n'.format(line))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-29T13:21:52.775619Z","iopub.execute_input":"2022-12-29T13:21:52.776219Z","iopub.status.idle":"2022-12-29T13:21:52.787711Z","shell.execute_reply.started":"2022-12-29T13:21:52.776176Z","shell.execute_reply":"2022-12-29T13:21:52.786679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Splits","metadata":{}},{"cell_type":"code","source":"label_frac = 1.0\nseed = 1\nk = 5\nval_frac = 0.15\ntest_frac = 0.15\nannot_frac = 0.4\nannot_positive_frac = 1\n\ndataset_csv_file = os.path.join('/kaggle/working/', 'fungal_vs_nonfungal.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:09:11.273044Z","iopub.execute_input":"2022-12-30T09:09:11.273708Z","iopub.status.idle":"2022-12-30T09:09:11.279597Z","shell.execute_reply.started":"2022-12-30T09:09:11.273673Z","shell.execute_reply":"2022-12-30T09:09:11.277973Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import os\nimport yaml\nimport random\nimport argparse\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:09:11.485955Z","iopub.execute_input":"2022-12-30T09:09:11.486274Z","iopub.status.idle":"2022-12-30T09:09:11.491387Z","shell.execute_reply.started":"2022-12-30T09:09:11.486243Z","shell.execute_reply":"2022-12-30T09:09:11.490278Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function, division\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport math\nimport re\nimport pdb\nimport pickle\nimport random\nfrom scipy import stats\n\nfrom torch.utils.data import Dataset\nimport h5py","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:09:11.772507Z","iopub.execute_input":"2022-12-30T09:09:11.773315Z","iopub.status.idle":"2022-12-30T09:09:11.778927Z","shell.execute_reply.started":"2022-12-30T09:09:11.773272Z","shell.execute_reply":"2022-12-30T09:09:11.778001Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def save_pkl(filename, save_object):\n    writer = open(filename,'wb')\n    pickle.dump(save_object, writer)\n    writer.close()\n\ndef load_pkl(filename):\n    loader = open(filename,'rb')\n    file = pickle.load(loader)\n    loader.close()\n    return file","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:09:11.997301Z","iopub.execute_input":"2022-12-30T09:09:11.997934Z","iopub.status.idle":"2022-12-30T09:09:12.003912Z","shell.execute_reply.started":"2022-12-30T09:09:11.997896Z","shell.execute_reply":"2022-12-30T09:09:12.002911Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def generate_split(cls_ids, val_num, test_num, samples, n_splits = 5,\n    seed = 7, label_frac = 1.0, custom_test_ids = None):\n    indices = np.arange(samples).astype(int)\n\n    if custom_test_ids is not None:\n        indices = np.setdiff1d(indices, custom_test_ids)\n\n    np.random.seed(seed)\n    for i in range(n_splits):\n        all_val_ids = []\n        all_test_ids = []\n        sampled_train_ids = []\n\n        if custom_test_ids is not None: # pre-built test split, do not need to sample\n            all_test_ids.extend(custom_test_ids)\n\n        for c in range(len(val_num)):\n            possible_indices = np.intersect1d(cls_ids[c], indices) #all indices of this class\n            val_ids = np.random.choice(possible_indices, val_num[c], replace = False) # validation ids\n\n            remaining_ids = np.setdiff1d(possible_indices, val_ids) #indices of this class left after validation\n            all_val_ids.extend(val_ids)\n\n            if custom_test_ids is None: # sample test split\n\n                test_ids = np.random.choice(remaining_ids, test_num[c], replace = False)\n                remaining_ids = np.setdiff1d(remaining_ids, test_ids)\n                all_test_ids.extend(test_ids)\n\n            if label_frac == 1:\n                sampled_train_ids.extend(remaining_ids)\n\n            else:\n                sample_num  = math.ceil(len(remaining_ids) * label_frac)\n                slice_ids = np.arange(sample_num)\n                sampled_train_ids.extend(remaining_ids[slice_ids])\n\n        yield sampled_train_ids, all_val_ids, all_test_ids\n\n\ndef nth(iterator, n, default=None):\n    if n is None:\n        return collections.deque(iterator, maxlen=0)\n    else:\n        return next(islice(iterator,n, None), default)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:09:12.287646Z","iopub.execute_input":"2022-12-30T09:09:12.288345Z","iopub.status.idle":"2022-12-30T09:09:12.301848Z","shell.execute_reply.started":"2022-12-30T09:09:12.288299Z","shell.execute_reply":"2022-12-30T09:09:12.300803Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def save_splits(split_datasets, column_keys, filename, annot_frac=None, annot_positive_frac=None, boolean_style=False, annot_create=True):\n    print(split_datasets)\n    splits = [split_datasets[i].slide_data['slide_id'] for i in range(len(split_datasets))]\n\n    if annot_create:\n        # Add annot column # Only for 2 classes\n        train_set = split_datasets[0]\n        train_set_list = []\n        annot_set = []\n        positive_list = []\n        negative_list = []\n\n        for ids in train_set.slide_cls_ids[0]:\n            negative_list.append(str(train_set.slide_data['slide_id'][ids]))\n\n        for ids in train_set.slide_cls_ids[1]:\n            positive_list.append(str(train_set.slide_data['slide_id'][ids]))\n\n        train_set_list.extend(negative_list)\n        train_set_list.extend(positive_list)\n\n        train_set_annot = np.round(len(train_set_list) * annot_frac)\n        neg_annot_num = np.round(train_set_annot * (1-annot_positive_frac)).astype(int)\n        pos_annot_num = np.round(train_set_annot * annot_positive_frac).astype(int)\n\n        neg_annot_set = random.sample(negative_list, neg_annot_num)\n        pos_annot_set = random.sample(positive_list, pos_annot_num)\n\n        annot_set.extend(neg_annot_set)\n        annot_set.extend(pos_annot_set)\n\n    #     print(\"annot_set\", annot_set)\n\n        true_annot_set = [False]*len(train_set_list)\n        for idx in range(len(true_annot_set)):\n            if train_set_list[idx] in annot_set:\n                true_annot_set[idx] = True\n    #     print(\"true_annot_set\", true_annot_set)\n        true_annot_set = pd.DataFrame(true_annot_set)\n    #     print(\"splits\", splits)\n    #     print(\"true_annot_set\", true_annot_set)\n        splits.insert(1, true_annot_set)\n\n    if not boolean_style:\n        df = pd.concat(splits, ignore_index=True, axis=1)\n        df.columns = column_keys\n    else:\n        df = pd.concat(splits, ignore_index = True, axis=0)\n        index = df.values.tolist()\n        one_hot = np.eye(len(split_datasets)).astype(bool)\n        bool_array = np.repeat(one_hot, [len(dset) for dset in split_datasets], axis=0)\n        df = pd.DataFrame(bool_array, index=index, columns = ['train', 'annot', 'val', 'test'])\n\n    print(split_datasets[0].slide_data)\n    df.to_csv(filename)\n    print()\n\nclass Generic_WSI_Classification_Dataset(Dataset):\n    def __init__(self,\n        csv_path = 'dataset_csv/ccrcc_clean.csv',\n        shuffle = False,\n        seed = 7,\n        print_info = True,\n        label_dict = {},\n        filter_dict = {},\n        ignore=[],\n        patient_strat=False,\n        label_col = None,\n        patient_voting = 'max',\n        results_dir = None\n        ):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            shuffle (boolean): Whether to shuffle\n            seed (int): random seed for shuffling the data\n            print_info (boolean): Whether to print a summary of the dataset\n            label_dict (dict): Dictionary with key, value pairs for converting str labels to int\n            ignore (list): List containing class labels to ignore\n        \"\"\"\n        self.label_dict = label_dict\n        self.num_classes = len(set(self.label_dict.values()))\n        self.seed = seed\n        self.print_info = print_info\n        self.patient_strat = patient_strat\n        self.train_ids, self.val_ids, self.test_ids  = (None, None, None)\n        self.data_dir = None\n        self.annot_dir = None\n        if not label_col:\n            label_col = 'label'\n        self.label_col = label_col\n\n        slide_data = pd.read_csv(csv_path)\n        slide_data = self.filter_df(slide_data, filter_dict)\n        slide_data = self.df_prep(slide_data, self.label_dict, ignore, self.label_col)\n        print(slide_data)\n\n        ###shuffle data\n        if shuffle:\n            np.random.seed(seed)\n            np.random.shuffle(slide_data)\n\n        self.slide_data = slide_data\n        if results_dir:\n            slide_data.to_csv(os.path.join(results_dir, 'dataset_csv.csv'))\n\n        self.patient_data_prep(patient_voting)\n        self.cls_ids_prep()\n\n        if print_info:\n            self.summarize()\n\n    def cls_ids_prep(self):\n        # store ids corresponding each class at the patient or case level\n        self.patient_cls_ids = [[] for i in range(self.num_classes)]\n        for i in range(self.num_classes):\n            self.patient_cls_ids[i] = np.where(self.patient_data['label'] == i)[0]\n\n        # store ids corresponding each class at the slide level\n        self.slide_cls_ids = [[] for i in range(self.num_classes)]\n        for i in range(self.num_classes):\n            self.slide_cls_ids[i] = np.where(self.slide_data['label'] == i)[0]\n\n    def patient_data_prep(self, patient_voting='max'):\n        patients = np.unique(np.array(self.slide_data['case_id'])) # get unique patients\n        patient_labels = []\n\n        for p in patients:\n            locations = self.slide_data[self.slide_data['case_id'] == p].index.tolist()\n            assert len(locations) > 0\n            label = self.slide_data['label'][locations].values\n            if patient_voting == 'max':\n                label = label.max() # get patient label (MIL convention)\n            elif patient_voting == 'maj':\n                label = stats.mode(label)[0]\n            else:\n                raise NotImplementedError\n            patient_labels.append(label)\n\n        self.patient_data = {'case_id':patients, 'label':np.array(patient_labels)}\n\n    @staticmethod\n    def df_prep(data, label_dict, ignore, label_col):\n        if label_col != 'label':\n            data['label'] = data[label_col].copy()\n\n        mask = data['label'].isin(ignore)\n        data = data[~mask]\n        data.reset_index(drop=True, inplace=True)\n        for i in data.index:\n            key = data.loc[i, 'label']\n            data.at[i, 'label'] = label_dict[key]\n\n        return data\n\n    def filter_df(self, df, filter_dict={}):\n        if len(filter_dict) > 0:\n            filter_mask = np.full(len(df), True, bool)\n            # assert 'label' not in filter_dict.keys()\n            for key, val in filter_dict.items():\n                mask = df[key].isin(val)\n                filter_mask = np.logical_and(filter_mask, mask)\n            df = df[filter_mask]\n        return df\n\n    def __len__(self):\n        if self.patient_strat:\n            return len(self.patient_data['case_id'])\n\n        else:\n            return len(self.slide_data)\n\n    def summarize(self):\n        print(\"label column: {}\".format(self.label_col))\n        print(\"label dictionary: {}\".format(self.label_dict))\n        print(\"number of classes: {}\".format(self.num_classes))\n        print(\"slide-level counts: \", '\\n', self.slide_data['label'].value_counts(sort = False))\n        for i in range(self.num_classes):\n            print('Patient-LVL; Number of samples registered in class %d: %d' % (i, self.patient_cls_ids[i].shape[0]))\n            print('Slide-LVL; Number of samples registered in class %d: %d' % (i, self.slide_cls_ids[i].shape[0]))\n\n    def create_splits(self, k = 3, val_num = (25, 25), test_num = (40, 40), label_frac = 1.0, custom_test_ids = None):\n        settings = {\n                    'n_splits' : k,\n                    'val_num' : val_num,\n                    'test_num': test_num,\n                    'label_frac': label_frac,\n                    'seed': self.seed,\n                    'custom_test_ids': custom_test_ids\n                    }\n\n        if self.patient_strat:\n            settings.update({'cls_ids' : self.patient_cls_ids, 'samples': len(self.patient_data['case_id'])})\n        else:\n            settings.update({'cls_ids' : self.slide_cls_ids, 'samples': len(self.slide_data)})\n\n        self.split_gen = generate_split(**settings)\n\n    def set_splits(self,start_from=None):\n        if start_from:\n            ids = nth(self.split_gen, start_from)\n\n        else:\n            ids = next(self.split_gen)\n\n        if self.patient_strat:\n            slide_ids = [[] for i in range(len(ids))]\n\n            for split in range(len(ids)):\n                for idx in ids[split]:\n                    case_id = self.patient_data['case_id'][idx]\n                    slide_indices = self.slide_data[self.slide_data['case_id'] == case_id].index.tolist()\n                    slide_ids[split].extend(slide_indices)\n\n            self.train_ids, self.val_ids, self.test_ids = slide_ids[0], slide_ids[1], slide_ids[2]\n\n        else:\n            self.train_ids, self.val_ids, self.test_ids = ids\n\n    def get_split_from_df(self, all_splits, split_key='train'):\n        split = all_splits[split_key]\n        split = split.dropna().reset_index(drop=True)\n\n        if len(split) > 0:\n            mask = self.slide_data['slide_id'].isin(split.tolist())\n            df_slice = self.slide_data[mask].reset_index(drop=True)\n            split = Generic_Split(df_slice, data_dir=self.data_dir, annot_dir=self.annot_dir, num_classes=self.num_classes)\n        else:\n            split = None\n\n        return split\n\n    def get_merged_split_from_df(self, all_splits, split_keys=['train']):\n        merged_split = []\n        for split_key in split_keys:\n            split = all_splits[split_key]\n            split = split.dropna().reset_index(drop=True).tolist()\n            merged_split.extend(split)\n\n        if len(split) > 0:\n            mask = self.slide_data['slide_id'].isin(merged_split)\n            df_slice = self.slide_data[mask].reset_index(drop=True)\n            split = Generic_Split(df_slice, data_dir=self.data_dir, num_classes=self.num_classes)\n        else:\n            split = None\n\n        return split\n\n    def get_overlap_split_from_df(self, all_splits, split_keys=['train', 'annot']):\n        train_split = all_splits['train']\n        annot_split = all_splits['annot']\n\n        if len(train_split) > 0:\n            mask = self.slide_data['slide_id'].isin(train_split)\n            df_slice = self.slide_data[mask].reset_index(drop=True)\n\n            mask = train_split.isin(df_slice['slide_id'].tolist())\n            df_slice['annot'] = annot_split[mask]\n\n            split = Generic_Split(df_slice, data_dir=self.data_dir, annot_dir=self.annot_dir, num_classes=self.num_classes)\n        else:\n            split = None\n\n        return split\n\n\n    def return_splits(self, from_id=True, csv_path=None):\n\n\n        if from_id:\n            if len(self.train_ids) > 0:\n                train_data = self.slide_data.loc[self.train_ids].reset_index(drop=True)\n                train_split = Generic_Split(train_data, annot_dir=self.annot_dir, data_dir=self.data_dir, num_classes=self.num_classes)\n\n            else:\n                train_split = None\n\n            if len(self.val_ids) > 0:\n                val_data = self.slide_data.loc[self.val_ids].reset_index(drop=True)\n                val_split = Generic_Split(val_data, data_dir=self.data_dir, num_classes=self.num_classes)\n\n            else:\n                val_split = None\n\n            if len(self.test_ids) > 0:\n                test_data = self.slide_data.loc[self.test_ids].reset_index(drop=True)\n                test_split = Generic_Split(test_data, data_dir=self.data_dir, num_classes=self.num_classes)\n\n            else:\n                test_split = None\n\n\n        else:\n            assert csv_path\n            all_splits = pd.read_csv(csv_path, dtype=self.slide_data['slide_id'].dtype)  # Without \"dtype=self.slide_data['slide_id'].dtype\", read_csv() will convert all-number columns to a numerical type. Even if we convert numerical columns back to objects later, we may lose zero-padding in the process; the columns must be correctly read in from the get-go. When we compare the individual train/val/test columns to self.slide_data['slide_id'] in the get_split_from_df() method, we cannot compare objects (strings) to numbers or even to incorrectly zero-padded objects/strings. An example of this breaking is shown in https://github.com/andrew-weisman/clam_analysis/tree/main/datatype_comparison_bug-2021-12-01.\n            train_split = self.get_overlap_split_from_df(all_splits, ['train', 'annot'])\n            val_split = self.get_overlap_split_from_df(all_splits, ['val', 'annot'])\n            test_split = self.get_overlap_split_from_df(all_splits, ['test', 'annot'])\n            # train_split = self.get_split_from_df(all_splits, 'train')\n            # val_split = self.get_split_from_df(all_splits, 'val')\n            # test_split = self.get_split_from_df(all_splits, 'test')\n\n        return train_split, val_split, test_split\n\n    def get_list(self, ids):\n        return self.slide_data['slide_id'][ids]\n\n    def getlabel(self, ids):\n        return self.slide_data['label'][ids]\n\n    def __getitem__(self, idx):\n        return None\n\n    def test_split_gen(self, return_descriptor=False):\n\n        if return_descriptor:\n            index = [list(self.label_dict.keys())[list(self.label_dict.values()).index(i)] for i in range(self.num_classes)]\n            columns = ['train', 'val', 'test']\n            df = pd.DataFrame(np.full((len(index), len(columns)), 0, dtype=np.int32), index= index,\n                            columns= columns)\n\n        count = len(self.train_ids)\n        print('\\nnumber of training samples: {}'.format(count))\n        labels = self.getlabel(self.train_ids)\n        unique, counts = np.unique(labels, return_counts=True)\n        for u in range(len(unique)):\n            print('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n            if return_descriptor:\n                df.loc[index[u], 'train'] = counts[u]\n\n        count = len(self.val_ids)\n        print('\\nnumber of val samples: {}'.format(count))\n        labels = self.getlabel(self.val_ids)\n        unique, counts = np.unique(labels, return_counts=True)\n        for u in range(len(unique)):\n            print('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n            if return_descriptor:\n                df.loc[index[u], 'val'] = counts[u]\n\n        count = len(self.test_ids)\n        print('\\nnumber of test samples: {}'.format(count))\n        labels = self.getlabel(self.test_ids)\n        unique, counts = np.unique(labels, return_counts=True)\n        for u in range(len(unique)):\n            print('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n            if return_descriptor:\n                df.loc[index[u], 'test'] = counts[u]\n\n        assert len(np.intersect1d(self.train_ids, self.test_ids)) == 0\n        assert len(np.intersect1d(self.train_ids, self.val_ids)) == 0\n        assert len(np.intersect1d(self.val_ids, self.test_ids)) == 0\n\n        if return_descriptor:\n            return df\n\n    def save_split(self, filename):\n        train_split = self.get_list(self.train_ids)\n        val_split = self.get_list(self.val_ids)\n        test_split = self.get_list(self.test_ids)\n        df_tr = pd.DataFrame({'train': train_split})\n        df_v = pd.DataFrame({'val': val_split})\n        df_t = pd.DataFrame({'test': test_split})\n        df = pd.concat([df_tr, df_v, df_t], axis=1)\n        df.to_csv(filename, index = False)\n\n\nclass Generic_MIL_Dataset(Generic_WSI_Classification_Dataset):\n    def __init__(self,\n        data_dir,\n        annot_dir=None,\n        **kwargs):\n\n        super(Generic_MIL_Dataset, self).__init__(**kwargs)\n        self.data_dir = data_dir\n        self.annot_dir = annot_dir\n        self.use_h5 = False\n\n    def load_from_h5(self, toggle):\n        self.use_h5 = toggle\n\n    def __getitem__(self, idx):\n        slide_id = self.slide_data['slide_id'][idx]\n        label = self.slide_data['label'][idx]\n        if self.slide_data['annot'][idx]:\n            bool_annot = bool(self.slide_data['annot'][idx])\n            if label == 1:\n                patch_annot_path = os.path.join(self.annot_dir, slide_id, slide_id+'.pkl')\n                patch_annot = load_pkl(patch_annot_path)\n                patch_annot = patch_annot['bin_scores']\n            elif label == 0:\n                patch_annot = [False]*24\n\n        if type(self.data_dir) == dict:\n            source = self.slide_data['source'][idx]\n            data_dir = self.data_dir[source]\n        else:\n            data_dir = self.data_dir\n\n        if not self.use_h5:\n            if self.data_dir:\n                full_path = os.path.join(data_dir, '{}.pt'.format(slide_id))\n                features = torch.load(full_path)\n                return features, label, bool_annot, patch_annot\n                # return features, label\n\n            else:\n                # if bool_annot:\n                #     return slide_id, label, bool_annot, patch_annot\n                # else:\n                #     return slide_id, label, None, None\n                return slide_id, label\n\n        else:\n            full_path = os.path.join(data_dir,'h5_files','{}.h5'.format(slide_id))\n            with h5py.File(full_path,'r') as hdf5_file:\n                features = hdf5_file['features'][:]\n                coords = hdf5_file['coords'][:]\n\n            features = torch.from_numpy(features)\n            return features, label, coords\n\n\nclass Generic_Split(Generic_MIL_Dataset):\n    def __init__(self, slide_data, annot_dir=None, data_dir=None, num_classes=2):\n        self.use_h5 = False\n        self.slide_data = slide_data\n        self.data_dir = data_dir\n        self.annot_dir = annot_dir\n        self.num_classes = num_classes\n        self.slide_cls_ids = [[] for i in range(self.num_classes)]\n        for i in range(self.num_classes):\n            self.slide_cls_ids[i] = np.where(self.slide_data['label'] == i)[0]\n\n    def __len__(self):\n        return len(self.slide_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:11:49.975323Z","iopub.execute_input":"2022-12-30T09:11:49.975700Z","iopub.status.idle":"2022-12-30T09:11:50.046440Z","shell.execute_reply.started":"2022-12-30T09:11:49.975667Z","shell.execute_reply":"2022-12-30T09:11:50.045279Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"random.seed(seed)\n\n# task_1_fungal_vs_nonfungal\nn_classes=2\ndataset = Generic_WSI_Classification_Dataset(csv_path = dataset_csv_file,\n                        shuffle = False,\n                        seed = seed,\n                        print_info = True,\n                        label_dict = {'nonfungal':0, 'fungal':1},\n                        patient_strat=True,\n                        ignore=[])\n\nnum_slides_cls = np.array([len(cls_ids) for cls_ids in dataset.patient_cls_ids])\nval_num = np.round(num_slides_cls * val_frac).astype(int)\ntest_num = np.round(num_slides_cls * test_frac).astype(int)\n\nif label_frac > 0:\n    label_fracs = [label_frac]\nelse:\n    label_fracs = [0.1, 0.25, 0.5, 0.75, 1.0]\n\nfor lf in label_fracs:\n    split_dir = 'splits/fungal_vs_nonfungal' + '_{}'.format(int(lf * 100))\n    os.makedirs(split_dir, exist_ok=True)\n    dataset.create_splits(k = k, val_num = val_num, test_num = test_num, label_frac=lf)\n    for i in range(k):\n        dataset.set_splits()\n        descriptor_df = dataset.test_split_gen(return_descriptor=True)\n        splits = dataset.return_splits(from_id=True)\n\n        save_splits(splits, ['train', 'annot', 'val', 'test'], os.path.join(split_dir, 'splits_{}.csv'.format(i)), annot_frac=annot_frac, annot_positive_frac=annot_positive_frac)\n        # save_splits(splits, ['train', 'annot', 'val', 'test'], os.path.join(split_dir, 'splits_{}_bool.csv'.format(i)), boolean_style=True)\n        # descriptor_df.to_csv(os.path.join(split_dir, 'splits_{}_descriptor.csv'.format(i)))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:11:50.488732Z","iopub.execute_input":"2022-12-30T09:11:50.489054Z","iopub.status.idle":"2022-12-30T09:11:51.578341Z","shell.execute_reply.started":"2022-12-30T09:11:50.489025Z","shell.execute_reply":"2022-12-30T09:11:51.577251Z"},"scrolled":true,"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"      case_id  slide_id label\n0      case_0   F005a02     1\n1      case_1   F006a01     1\n2      case_2   F006a02     1\n3      case_3   F006a03     1\n4      case_4   F006a04     1\n..        ...       ...   ...\n418  case_418  N012a017     0\n419  case_419  N012a018     0\n420  case_420  N012a019     0\n421  case_421  N012a020     0\n422  case_422  N017a001     0\n\n[423 rows x 3 columns]\nlabel column: label\nlabel dictionary: {'nonfungal': 0, 'fungal': 1}\nnumber of classes: 2\nslide-level counts:  \n 1    208\n0    215\nName: label, dtype: int64\nPatient-LVL; Number of samples registered in class 0: 215\nSlide-LVL; Number of samples registered in class 0: 215\nPatient-LVL; Number of samples registered in class 1: 208\nSlide-LVL; Number of samples registered in class 1: 208\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7fab30367d10>, <__main__.Generic_Split object at 0x7fab301a2b50>, <__main__.Generic_Split object at 0x7fab301a2b90>)\n      case_id  slide_id label\n0    case_208  N001a001     0\n1    case_209  N001a002     0\n2    case_210  N003a001     0\n3    case_211  N003a002     0\n4    case_217  N004a005     0\n..        ...       ...   ...\n292   case_94   F030a06     1\n293   case_95   F030a07     1\n294   case_96   F030a08     1\n295   case_98   F030a10     1\n296   case_99   F030a11     1\n\n[297 rows x 3 columns]\n\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7fab30a07fd0>, <__main__.Generic_Split object at 0x7fab301a2950>, <__main__.Generic_Split object at 0x7fab301a2290>)\n      case_id  slide_id label\n0    case_208  N001a001     0\n1    case_210  N003a001     0\n2    case_211  N003a002     0\n3    case_212  N003a003     0\n4    case_215  N004a003     0\n..        ...       ...   ...\n292   case_94   F030a06     1\n293   case_95   F030a07     1\n294   case_96   F030a08     1\n295   case_97   F030a09     1\n296   case_99   F030a11     1\n\n[297 rows x 3 columns]\n\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7fab30367d10>, <__main__.Generic_Split object at 0x7fabf31be2d0>, <__main__.Generic_Split object at 0x7fab301a2450>)\n      case_id  slide_id label\n0    case_208  N001a001     0\n1    case_209  N001a002     0\n2    case_212  N003a003     0\n3    case_213  N004a001     0\n4    case_215  N004a003     0\n..        ...       ...   ...\n292   case_93   F030a05     1\n293   case_95   F030a07     1\n294   case_96   F030a08     1\n295   case_97   F030a09     1\n296   case_98   F030a10     1\n\n[297 rows x 3 columns]\n\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7fab30a07fd0>, <__main__.Generic_Split object at 0x7fab302b3c10>, <__main__.Generic_Split object at 0x7fab301a2ad0>)\n      case_id  slide_id label\n0    case_208  N001a001     0\n1    case_210  N003a001     0\n2    case_211  N003a002     0\n3    case_213  N004a001     0\n4    case_215  N004a003     0\n..        ...       ...   ...\n292   case_92   F030a04     1\n293   case_93   F030a05     1\n294   case_95   F030a07     1\n295   case_97   F030a09     1\n296   case_99   F030a11     1\n\n[297 rows x 3 columns]\n\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7fab30367d10>, <__main__.Generic_Split object at 0x7fab302b3ad0>, <__main__.Generic_Split object at 0x7fab301a2990>)\n      case_id  slide_id label\n0    case_209  N001a002     0\n1    case_210  N003a001     0\n2    case_211  N003a002     0\n3    case_213  N004a001     0\n4    case_214  N004a002     0\n..        ...       ...   ...\n292   case_94   F030a06     1\n293   case_95   F030a07     1\n294   case_96   F030a08     1\n295   case_97   F030a09     1\n296   case_98   F030a10     1\n\n[297 rows x 3 columns]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# View the splits\n# !cat splits/fungal_vs_nonfungal_100/splits_0.csv","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:15:05.166871Z","iopub.execute_input":"2022-12-30T09:15:05.167314Z","iopub.status.idle":"2022-12-30T09:15:05.172429Z","shell.execute_reply.started":"2022-12-30T09:15:05.167279Z","shell.execute_reply":"2022-12-30T09:15:05.171351Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}