{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Create Patches","metadata":{}},{"cell_type":"code","source":"function_type = 'tile_annotations'  # ['tile', 'tile_annotations', 'artefact_annotations']\n\nif function_type == 'tile':\n    # Regular patching\n    source_dir = os.path.abspath(\"/kaggle/input/fungal-10x/\")\n    patch_dir = os.path.abspath('/kaggle/working/patches/')\nelif function_type == 'tile_annotations':\n    # Patching for annotations\n    source_dir = os.path.abspath(\"/kaggle/input/fungal-10x-annot/\")\n    patch_dir = os.path.abspath('/kaggle/working/patch_annot/')\n    \npatch_size = 256\nthresholds = {\n    'annotations': 254,  # For the annotated images\n    'artefacts': 50,  # For artefacts in all images\n    'patch_positive': 10000  # For positive label\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-03T08:28:05.107507Z","iopub.execute_input":"2023-01-03T08:28:05.107879Z","iopub.status.idle":"2023-01-03T08:28:05.114704Z","shell.execute_reply.started":"2023-01-03T08:28:05.107850Z","shell.execute_reply":"2023-01-03T08:28:05.113597Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport argparse\nimport yaml\nimport cv2\nimport pickle\nimport numpy as np\nfrom PIL import Image\nfrom itertools import product\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:49:51.233436Z","iopub.execute_input":"2023-01-03T11:49:51.233794Z","iopub.status.idle":"2023-01-03T11:49:51.239451Z","shell.execute_reply.started":"2023-01-03T11:49:51.233764Z","shell.execute_reply":"2023-01-03T11:49:51.238390Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def save_pkl(filename, save_object):\n    writer = open(filename,'wb')\n    pickle.dump(save_object, writer)\n    writer.close()\n\ndef load_pkl(filename):\n    loader = open(filename,'rb')\n    file = pickle.load(loader)\n    loader.close()\n    return file","metadata":{"execution":{"iopub.status.busy":"2023-01-03T08:28:08.106684Z","iopub.execute_input":"2023-01-03T08:28:08.107070Z","iopub.status.idle":"2023-01-03T08:28:08.112999Z","shell.execute_reply.started":"2023-01-03T08:28:08.107039Z","shell.execute_reply":"2023-01-03T08:28:08.111870Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def tile(filename, dir_in, dir_out, d):\n    if not os.path.isdir(dir_out):\n        os.mkdir(dir_out)\n\n    name, ext = os.path.splitext(filename)\n    img = Image.open(os.path.join(dir_in, filename))\n    w, h = img.size\n\n    grid = product(range(0, h-h%d, d), range(0, w-w%d, d))\n    for i, j in grid:\n        box = (j, i, j+d, i+d)\n        i /= 256\n        j /= 256\n        out = os.path.join(dir_out, f'{name}_{int(i)}_{int(j)}{ext}')\n        img.crop(box).save(out)\n\n        \ndef tile_annotations(filename, dir_in, dir_out, d):\n    if not os.path.isdir(dir_out):\n        os.mkdir(dir_out)\n\n    patch_scores = []\n    name, ext = os.path.splitext(filename)\n    img_cv = cv2.imread(os.path.join(dir_in, filename))\n    img_cv_gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n    # Thresholding options: ['THRESH_BINARY', 'THRESH_BINARY_INV', 'THRESH_TOZERO ', 'THRESH_TOZERO_INV', 'THRESH_OTSU']\n    ret, img_cv_binarized = cv2.threshold(img_cv_gray, thresholds['annotations'], 255, cv2.THRESH_TOZERO)  # Apply thresholding\n    img_pil_binarized = cv2.cvtColor(img_cv_binarized, cv2.COLOR_BGR2RGB)  # Convert to RGB, for PIL Image\n    img_pil_binarized = Image.fromarray(img_pil_binarized)  # Convert to PIL Image\n    w, h = img_pil_binarized.size\n\n    grid = product(range(0, h-h%d, d), range(0, w-w%d, d))\n    for i, j in grid:\n        box = (j, i, j+d, i+d)\n        i /= 256\n        j /= 256\n        out = os.path.join(dir_out, f'{name}_{int(i)}_{int(j)}{ext}')\n\n        img_patch = img_pil_binarized.crop(box)\n\n        img_patch_np = np.asarray(img_patch)  # Convert to Numpy array\n        patch_non_zero = np.count_nonzero(img_patch_np)\n        patch_scores.append(patch_non_zero)\n\n        img_patch.save(out)  # Save patch image\n\n    print(\"P\", patch_scores)\n\n    bin_scores = []\n    for score in patch_scores:\n        bin_score = (score > thresholds['patch_positive']) if 1 else 0\n        bin_scores.append(bin_score)\n\n    save_path = os.path.join(dir_out, name+\".pkl\")\n    save_object = {\n        \"patch_scores\": patch_scores,\n        \"bin_scores\": bin_scores\n    }\n    save_pkl(save_path, save_object)","metadata":{"scrolled":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-01-03T08:28:08.588191Z","iopub.execute_input":"2023-01-03T08:28:08.588556Z","iopub.status.idle":"2023-01-03T08:28:08.603398Z","shell.execute_reply.started":"2023-01-03T08:28:08.588524Z","shell.execute_reply":"2023-01-03T08:28:08.602353Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if not os.path.isdir(patch_dir):\n    os.mkdir(patch_dir)\n\nfor filename in os.listdir(source_dir):\n    name, ext = os.path.splitext(filename)\n    output_patches_dir = os.path.join(patch_dir, name)\n\n    if function_type == 'tile':\n        print(\"Patching\", filename)\n        tile(filename, source_dir, output_patches_dir, patch_size)\n    elif function_type == 'tile_annotations':\n        print(\"Binarizing and Patching Annotated\", filename)\n        tile_annotations(filename, source_dir, output_patches_dir, patch_size)\n    elif function_type == 'artefact_annotations':\n        print(\"Binarizing and Patching Artefacts\", filename)\n        artefact_annotations(filename, source_dir, patch_dir, patch_size)\n    else:\n        print(\"Unknown function_type\")","metadata":{"scrolled":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-12-30T10:43:57.103487Z","iopub.execute_input":"2022-12-30T10:43:57.104128Z","iopub.status.idle":"2022-12-30T10:44:18.284055Z","shell.execute_reply.started":"2022-12-30T10:43:57.104079Z","shell.execute_reply":"2022-12-30T10:44:18.282495Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Binarizing and Patching Annotated F052a17.tif\nP [1758, 0, 0, 0, 11256, 15765, 9333, 13179, 5532, 0, 636, 1410, 5640, 13674, 1188, 0, 0, 0, 5136, 606, 915, 0, 0, 0]\nBinarizing and Patching Annotated F021a01.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 261, 0, 0, 0, 0, 8172, 8367, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F033a12.tif\nP [34302, 24297, 36282, 21402, 690, 0, 20457, 14700, 23901, 27651, 14691, 1845, 14886, 7566, 2319, 21015, 8235, 6045, 20211, 14979, 9825, 1494, 5793, 1683]\nBinarizing and Patching Annotated F017a09.tif\nP [2847, 3048, 0, 7785, 111, 0, 0, 0, 0, 6843, 8436, 729, 0, 0, 2583, 8607, 8862, 7611, 5643, 9033, 4644, 1890, 2610, 3459]\nBinarizing and Patching Annotated F052a06.tif\nP [5025, 1881, 4518, 7605, 1320, 975, 8499, 17136, 14025, 15303, 9891, 0, 5487, 16491, 13971, 12336, 9390, 1515, 16680, 20250, 7707, 1338, 14547, 4347]\nBinarizing and Patching Annotated F053a01.tif\nP [0, 0, 0, 0, 0, 150, 399, 0, 0, 3, 0, 5676, 396, 78, 0, 1914, 2184, 7788, 39, 234, 2862, 5040, 10392, 0]\nBinarizing and Patching Annotated F007a08.tif\nP [7707, 7701, 18663, 25002, 23439, 3534, 4008, 16593, 22179, 11877, 23898, 10101, 13224, 28593, 14940, 13998, 13602, 29841, 32631, 23595, 7431, 17154, 26217, 13410]\nBinarizing and Patching Annotated F058a05.tif\nP [20829, 19458, 111, 927, 12465, 0, 17559, 25488, 7767, 885, 303, 63, 0, 0, 0, 15789, 27129, 1833, 0, 0, 0, 288, 8388, 477]\nBinarizing and Patching Annotated F018a10.tif\nP [0, 0, 0, 0, 0, 0, 0, 3945, 4752, 2049, 0, 0, 3132, 1449, 441, 0, 0, 0, 1206, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F018a06.tif\nP [0, 1107, 0, 3798, 0, 0, 0, 0, 2334, 4053, 3660, 0, 0, 3420, 22323, 5520, 1842, 0, 0, 2565, 10428, 9930, 2841, 0]\nBinarizing and Patching Annotated F013a11.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2499, 1566, 0, 0, 0, 5931, 1563, 0, 0, 0, 0, 1356, 1536, 0]\nBinarizing and Patching Annotated F030a08.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3285, 4905, 339, 1935, 0]\nBinarizing and Patching Annotated F056a18.tif\nP [0, 1986, 5343, 4494, 0, 0, 0, 633, 1998, 6318, 1503, 0, 0, 0, 8745, 5847, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F006a03.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 888, 0, 2079, 0, 0, 0, 0, 0, 2850, 0, 1122, 2025, 0, 0, 0, 0]\nBinarizing and Patching Annotated F056a09.tif\nP [0, 942, 0, 2571, 0, 4179, 8142, 3120, 1194, 5175, 3678, 894, 11865, 7671, 5463, 5070, 1137, 0, 4257, 1938, 0, 0, 5625, 2232]\nBinarizing and Patching Annotated F033a29.tif\nP [6000, 18111, 18219, 31614, 25437, 9438, 12324, 12552, 13311, 11709, 21966, 8529, 10524, 13560, 11310, 19719, 16020, 13092, 20436, 20130, 4872, 6099, 15471, 8310]\nBinarizing and Patching Annotated F053a09.tif\nP [2868, 9369, 11346, 8292, 0, 0, 0, 2772, 6003, 6087, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F053a04.tif\nP [6996, 8178, 345, 0, 3, 0, 6768, 1461, 4257, 1404, 0, 0, 1677, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F013a02.tif\nP [0, 3603, 5151, 2133, 4257, 2004, 4479, 4545, 0, 4158, 2736, 0, 0, 3690, 8343, 9519, 4056, 0, 0, 0, 0, 3489, 0, 0]\nBinarizing and Patching Annotated F030a09.tif\nP [69, 2364, 0, 0, 0, 0, 0, 4119, 0, 0, 0, 0, 0, 0, 0, 2490, 0, 0, 0, 0, 0, 297, 12, 0]\nBinarizing and Patching Annotated F007a05.tif\nP [12741, 19056, 32010, 30189, 14397, 9981, 10002, 29928, 22965, 28542, 9144, 0, 6270, 16488, 15891, 30786, 25461, 1731, 28611, 20664, 19560, 23316, 19305, 2853]\nBinarizing and Patching Annotated F030a13.tif\nP [0, 0, 894, 75, 4179, 10629, 3222, 0, 9978, 2034, 14682, 6366, 6324, 4587, 7470, 8562, 7998, 4134, 10089, 14556, 6450, 2073, 423, 0]\nBinarizing and Patching Annotated F007a15.tif\nP [14280, 10734, 12513, 26511, 10665, 14865, 12930, 24459, 27846, 14325, 11280, 16230, 4626, 4218, 5874, 0, 10707, 23676, 0, 0, 0, 0, 3441, 17706]\nBinarizing and Patching Annotated F006a07.tif\nP [0, 2598, 1308, 0, 0, 0, 0, 2577, 9831, 3594, 0, 0, 0, 0, 1992, 0, 0, 0, 4449, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F058a01.tif\nP [0, 0, 0, 0, 0, 1758, 0, 0, 0, 8895, 1206, 1323, 0, 0, 3351, 38202, 19329, 1557, 0, 0, 9522, 4953, 576, 0]\nBinarizing and Patching Annotated F007a12.tif\nP [20979, 11268, 17424, 21447, 0, 2751, 8208, 22206, 13392, 8943, 1638, 3951, 26523, 12579, 8406, 12234, 0, 0, 29385, 22356, 8055, 11442, 17892, 2055]\nBinarizing and Patching Annotated F006a01.tif\nP [0, 0, 0, 570, 0, 0, 0, 0, 3369, 6831, 735, 0, 0, 0, 6813, 6093, 0, 0, 0, 0, 3069, 3645, 0, 0]\nBinarizing and Patching Annotated F013a09.tif\nP [471, 9675, 0, 4110, 1101, 0, 1041, 2421, 114, 2763, 4386, 0, 6528, 0, 0, 9180, 9303, 2382, 0, 0, 0, 9084, 2979, 522]\nBinarizing and Patching Annotated F007a06.tif\nP [12483, 14940, 3819, 23634, 15792, 0, 3132, 29805, 19314, 20058, 9948, 0, 18324, 15576, 17901, 24714, 27246, 10248, 21936, 21342, 19131, 23856, 21015, 6183]\nBinarizing and Patching Annotated F033a02.tif\nP [0, 0, 0, 0, 0, 0, 2784, 0, 0, 0, 0, 0, 1596, 0, 342, 3267, 0, 0, 0, 1368, 4503, 10560, 0, 0]\nBinarizing and Patching Annotated F030a05.tif\nP [0, 2397, 0, 0, 1689, 2532, 4077, 2133, 0, 0, 0, 1665, 1467, 2406, 0, 0, 0, 0, 3414, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F013a10.tif\nP [84, 10572, 3624, 4038, 1080, 0, 1155, 2715, 1269, 2730, 4398, 0, 7011, 0, 0, 9417, 8877, 420, 0, 0, 0, 8943, 2769, 0]\nBinarizing and Patching Annotated F053a19.tif\nP [180, 2802, 4443, 5895, 8034, 7134, 300, 16344, 8373, 13410, 4380, 2211, 7461, 2274, 3828, 5763, 0, 0, 6453, 13218, 8478, 11073, 0, 0]\nBinarizing and Patching Annotated F007a17.tif\nP [29256, 21981, 21951, 0, 2655, 3978, 15417, 6333, 3051, 2097, 0, 10833, 13044, 17877, 7965, 10773, 8628, 9036, 6183, 10866, 5871, 741, 3447, 1299]\nBinarizing and Patching Annotated F033a21.tif\nP [7527, 8340, 5025, 8574, 21048, 8718, 8748, 15279, 15012, 9270, 14232, 15432, 0, 1446, 4473, 9735, 8298, 4695, 0, 816, 5460, 8073, 14598, 6078]\nBinarizing and Patching Annotated F030a17.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 4185, 948, 0, 774, 2559, 1863, 17565, 10299, 13779, 2007, 5790, 12174, 5070, 2340, 18225, 10863]\nBinarizing and Patching Annotated F052a03.tif\nP [0, 3486, 2418, 0, 0, 0, 5607, 0, 2277, 0, 0, 0, 7854, 0, 0, 4242, 2718, 180, 5118, 5373, 13071, 16605, 13647, 5346]\nBinarizing and Patching Annotated F056a15.tif\nP [0, 117, 4455, 15897, 11721, 11364, 1854, 10242, 9288, 8892, 8574, 6234, 18240, 12180, 24630, 8547, 6909, 3672, 14013, 32469, 18216, 7233, 2325, 765]\nBinarizing and Patching Annotated F052a12.tif\nP [20427, 8757, 0, 10329, 7821, 9390, 14472, 9366, 0, 8040, 36, 2430, 18096, 15339, 4272, 3171, 0, 0, 6135, 1005, 4458, 0, 0, 0]\nBinarizing and Patching Annotated F018a02.tif\nP [0, 0, 0, 855, 480, 249, 0, 2040, 6750, 51, 4584, 13311, 0, 459, 22005, 13317, 5604, 1629, 0, 0, 2427, 3123, 7305, 0]\nBinarizing and Patching Annotated F010a02.tif\nP [0, 0, 0, 0, 0, 0, 2943, 1668, 11520, 4269, 0, 0, 3150, 4536, 5745, 0, 0, 0, 0, 405, 2193, 4254, 0, 0]\nBinarizing and Patching Annotated F052a13.tif\nP [6792, 14592, 17307, 3393, 1365, 5751, 18810, 20454, 10731, 2037, 0, 1791, 4716, 1980, 5043, 231, 417, 4806, 0, 0, 0, 0, 1470, 1896]\nBinarizing and Patching Annotated F052a09.tif\nP [18, 9, 5013, 3252, 651, 11724, 3, 0, 555, 5679, 3264, 21480, 0, 0, 0, 4044, 13530, 22593, 1857, 6312, 408, 975, 12669, 18189]\nBinarizing and Patching Annotated F052a11.tif\nP [21225, 10098, 0, 11445, 7557, 6579, 15333, 10167, 0, 7893, 579, 3003, 18870, 15483, 2898, 2472, 1251, 0, 4035, 960, 4488, 0, 0, 0]\nBinarizing and Patching Annotated F053a15.tif\nP [6054, 2904, 6657, 1671, 0, 0, 18408, 16314, 14121, 8382, 984, 6243, 11547, 22452, 26895, 23124, 30261, 16080, 1596, 14316, 6891, 20259, 29283, 15390]\nBinarizing and Patching Annotated F007a19.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 3387, 12192, 2478, 0, 1398, 0, 5421, 17580, 2667, 0, 0, 0, 0, 906, 1767, 0]\nBinarizing and Patching Annotated F018a07.tif\nP [0, 8460, 13611, 3024, 0, 0, 0, 4875, 3657, 6837, 471, 0, 0, 7371, 16983, 11802, 1161, 0, 0, 696, 6303, 2775, 0, 0]\nBinarizing and Patching Annotated F052a07.tif\nP [3066, 14085, 7833, 6984, 3354, 15879, 0, 11841, 5442, 4203, 12837, 17301, 4227, 2898, 435, 11499, 15636, 12291, 0, 5436, 2385, 17655, 27069, 15540]\nBinarizing and Patching Annotated F033a05.tif\nP [8547, 5226, 11940, 2607, 0, 0, 21123, 19308, 29745, 28617, 4668, 0, 18552, 36762, 41631, 35682, 18996, 1257, 20316, 31437, 15081, 24525, 19269, 6000]\nBinarizing and Patching Annotated F013a04.tif\nP [0, 4482, 4140, 0, 1203, 5721, 0, 642, 39, 0, 5691, 1503, 0, 4149, 4251, 0, 0, 2775, 7620, 8118, 7740, 0, 531, 5961]\nBinarizing and Patching Annotated F013a08.tif\nP [0, 0, 0, 0, 0, 0, 0, 1482, 3600, 0, 0, 0, 0, 0, 6387, 435, 0, 0, 0, 0, 1314, 3690, 0, 0]\nBinarizing and Patching Annotated F053a05.tif\nP [7266, 8553, 33, 0, 0, 0, 3480, 2205, 5355, 1137, 0, 0, 1599, 912, 876, 0, 0, 0, 0, 1782, 0, 0, 0, 0]\nBinarizing and Patching Annotated F033a10.tif\nP [21621, 5415, 534, 0, 0, 0, 3636, 4728, 13395, 156, 0, 0, 0, 1512, 19545, 10716, 9393, 5340, 0, 135, 7659, 13089, 6429, 13743]\nBinarizing and Patching Annotated F058a03.tif\nP [0, 1872, 4320, 0, 0, 0, 0, 0, 0, 4176, 1116, 0, 0, 0, 0, 3462, 14787, 31314, 0, 0, 1617, 519, 4113, 8088]\nBinarizing and Patching Annotated F050a03.tif\nP [357, 6, 0, 0, 0, 0, 18240, 2865, 0, 0, 0, 0, 7158, 11169, 5283, 6, 0, 0, 1788, 3519, 5283, 0, 0, 0]\nBinarizing and Patching Annotated F007a11.tif\nP [0, 10197, 13989, 6258, 2271, 9324, 4167, 13272, 8925, 10473, 6219, 4425, 0, 1146, 0, 1497, 4083, 11076, 0, 0, 0, 0, 5058, 66]\nBinarizing and Patching Annotated F053a03.tif\nP [0, 630, 16896, 15951, 240, 0, 936, 5934, 16122, 10347, 1986, 0, 9021, 1680, 1782, 3543, 0, 0, 6552, 2391, 2181, 1122, 0, 0]\nBinarizing and Patching Annotated F018a08.tif\nP [0, 0, 10965, 15510, 37059, 29868, 0, 10701, 27117, 21354, 35178, 32394, 1716, 19011, 23406, 25518, 18600, 21852, 19365, 22629, 18942, 20163, 10041, 23121]\nBinarizing and Patching Annotated F007a09.tif\nP [16299, 15660, 19749, 21174, 6120, 4398, 8610, 18708, 19083, 7869, 165, 1155, 6156, 10524, 18747, 4548, 0, 0, 10176, 10644, 11778, 15498, 5424, 2511]\nBinarizing and Patching Annotated F033a22.tif\nP [10635, 11475, 20121, 12270, 14448, 8724, 6153, 14967, 21027, 23742, 11109, 12375, 11565, 13398, 16251, 28305, 17925, 13530, 18312, 24339, 25959, 27978, 31398, 21723]\nBinarizing and Patching Annotated F033a15.tif\nP [9684, 11256, 26529, 23724, 14163, 16674, 5196, 17991, 27294, 27630, 11766, 3084, 22935, 27846, 31014, 9825, 24543, 16887, 30324, 12867, 14670, 5040, 4038, 7509]\nBinarizing and Patching Annotated F034a06.tif\nP [22512, 19077, 26325, 8562, 10383, 0, 34587, 21174, 15927, 18813, 28116, 2547, 9486, 28203, 17301, 36147, 59919, 13887, 20853, 19737, 26355, 15594, 38253, 2745]\nBinarizing and Patching Annotated F033a09.tif\nP [13383, 2364, 708, 0, 0, 0, 3798, 3768, 13746, 177, 0, 0, 162, 1461, 15933, 10692, 7113, 4242, 0, 60, 3297, 11664, 9750, 12594]\nBinarizing and Patching Annotated F052a18.tif\nP [1938, 0, 0, 0, 8319, 13623, 9369, 7755, 3396, 0, 0, 1548, 6381, 12795, 39, 0, 0, 0, 4680, 453, 957, 0, 0, 0]\nBinarizing and Patching Annotated F056a16.tif\nP [0, 0, 2448, 10011, 6741, 0, 0, 0, 633, 11709, 4590, 0, 0, 330, 4974, 3636, 606, 0, 0, 0, 0, 2295, 0, 0]\nBinarizing and Patching Annotated F007a21.tif\nP [15999, 15978, 18399, 27708, 22809, 228, 16512, 10299, 12630, 15186, 12612, 7515, 7449, 19773, 9153, 8238, 3246, 14442, 5280, 11493, 18699, 6054, 1158, 888]\nBinarizing and Patching Annotated F034a11.tif\nP [0, 0, 75, 1263, 8253, 11679, 498, 9084, 5145, 8163, 4002, 7719, 1128, 6150, 5967, 18702, 17463, 13905, 0, 4104, 6285, 4866, 3363, 4911]\nBinarizing and Patching Annotated F033a07.tif\nP [3204, 4182, 15432, 7950, 11598, 16074, 6591, 12621, 15108, 17322, 17865, 16071, 3309, 18330, 15831, 19800, 17592, 15012, 2619, 7461, 9084, 29007, 16494, 15834]\nBinarizing and Patching Annotated F009a01.tif\nP [0, 0, 2343, 5277, 0, 0, 0, 0, 1626, 3156, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F033a25.tif\nP [10953, 28005, 14625, 13674, 15225, 32967, 33663, 21357, 20301, 22218, 27360, 32985, 37050, 20622, 22167, 8322, 17022, 17835, 30912, 10773, 12585, 11637, 8019, 4116]\nBinarizing and Patching Annotated F010a03.tif\nP [0, 3786, 8574, 11331, 42, 2322, 0, 1902, 1290, 8535, 11760, 11271, 0, 0, 0, 12060, 4494, 1263, 0, 0, 501, 9336, 10518, 849]\nBinarizing and Patching Annotated F007a18.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 4842, 2106, 0, 1575, 2514, 1371, 8736, 14823, 1149, 0, 54, 57, 0, 7674, 1290, 0]\nBinarizing and Patching Annotated F017a01.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 492, 1974, 0, 0, 0, 0, 828, 5118, 2295, 0, 0, 3678, 0, 0, 0, 0]\nBinarizing and Patching Annotated F030a06.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 513, 4200, 2961, 1986, 0, 0]\nBinarizing and Patching Annotated F013a15.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16971, 6444, 0, 0, 0, 0, 2814, 4023, 0]\nBinarizing and Patching Annotated F009a04.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 213, 6687, 318, 0, 0, 0, 0, 1791, 3261, 0]\nBinarizing and Patching Annotated F010a01.tif\nP [0, 1149, 2892, 0, 2133, 11208, 0, 0, 6306, 5160, 13839, 2223, 0, 5130, 204, 8313, 8634, 6912, 510, 12246, 13422, 6750, 0, 0]\nBinarizing and Patching Annotated F033a16.tif\nP [5697, 6591, 23418, 21651, 12690, 14748, 3822, 14781, 25818, 21534, 9741, 1944, 21429, 25032, 28149, 8349, 19386, 8334, 28752, 10719, 11649, 0, 390, 2886]\nBinarizing and Patching Annotated F050a01.tif\nP [0, 240, 3144, 0, 12, 6012, 9, 5178, 14259, 1671, 2481, 189, 0, 12, 6729, 186, 15444, 2175, 276, 72, 1347, 1548, 2184, 93]\nBinarizing and Patching Annotated F013a07.tif\nP [0, 0, 0, 4662, 4203, 111, 0, 0, 0, 126, 0, 0, 0, 0, 0, 1578, 3525, 1953, 0, 0, 1422, 6915, 0, 0]\nBinarizing and Patching Annotated F030a14.tif\nP [8496, 7896, 0, 2037, 0, 1755, 1929, 2148, 0, 5400, 7896, 9672, 0, 2529, 6627, 9216, 8331, 7278, 0, 3783, 13998, 13785, 2781, 1026]\nBinarizing and Patching Annotated F030a04.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 1872, 0, 0, 0, 0, 0, 849, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F053a08.tif\nP [234, 1488, 6657, 6576, 918, 1602, 24, 5931, 5886, 8253, 660, 0, 5694, 5193, 8067, 6414, 0, 0, 1632, 12084, 11991, 6561, 0, 0]\nBinarizing and Patching Annotated F021a02.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 63, 1098, 0, 0, 0, 2088, 1854, 1134, 0, 0, 0, 3168, 8499, 0, 0]\nBinarizing and Patching Annotated F053a12.tif\nP [0, 0, 4962, 1776, 9450, 7323, 3, 1314, 8661, 12165, 5376, 8046, 0, 22152, 13395, 9312, 4224, 3042, 9222, 10359, 3780, 0, 0, 0]\nBinarizing and Patching Annotated F010a05.tif\nP [0, 0, 504, 3777, 6012, 351, 0, 0, 17082, 6489, 19038, 7545, 0, 7140, 21669, 18435, 4164, 2808, 0, 0, 15948, 11715, 4908, 0]\nBinarizing and Patching Annotated F030a07.tif\nP [0, 0, 0, 0, 2559, 1653, 0, 0, 0, 0, 2016, 3432, 0, 0, 0, 0, 4743, 195, 0, 222, 0, 0, 2853, 0]\nBinarizing and Patching Annotated F052a05.tif\nP [4434, 1896, 5085, 8376, 396, 0, 8502, 16926, 15087, 15147, 12243, 0, 5718, 17058, 14295, 14160, 6780, 1986, 18369, 19242, 7827, 1410, 15273, 5409]\nBinarizing and Patching Annotated F006a08.tif\nP [0, 165, 1266, 0, 0, 0, 0, 2820, 8154, 0, 0, 0, 0, 0, 3153, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F018a04.tif\nP [0, 0, 0, 4377, 0, 0, 0, 0, 8346, 4308, 4899, 0, 0, 7236, 25308, 1542, 0, 0, 0, 3099, 9633, 8640, 9633, 753]\nBinarizing and Patching Annotated F056a13.tif\nP [10371, 7338, 13662, 16716, 18816, 35730, 5796, 9405, 11415, 18960, 17556, 13797, 6636, 6795, 11217, 14376, 6249, 1881, 9618, 10902, 18177, 7533, 2754, 2814]\nBinarizing and Patching Annotated F015a01.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14043, 15429, 0, 0, 0, 0, 11193, 1386, 0, 0]\nBinarizing and Patching Annotated F056a08.tif\nP [0, 0, 0, 1191, 0, 0, 8085, 1113, 0, 3543, 5193, 0, 7923, 7149, 4305, 2034, 0, 0, 4098, 1698, 0, 0, 5412, 3486]\nBinarizing and Patching Annotated F018a11.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6984, 2724, 0, 0, 0, 3720, 6963, 7908, 918]\nBinarizing and Patching Annotated F012a04.tif\nP [0, 0, 3195, 474, 6003, 5886, 0, 2103, 3669, 6384, 84, 5352, 0, 1677, 17391, 7581, 0, 153, 0, 3330, 9732, 6858, 144, 0]\nBinarizing and Patching Annotated F033a26.tif\nP [0, 10911, 12279, 3375, 2895, 555, 6213, 27399, 11868, 20061, 6672, 12780, 5871, 19044, 18639, 19431, 12171, 12195, 10494, 15939, 23016, 8256, 20634, 15147]\nBinarizing and Patching Annotated F018a12.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8391, 2721, 0, 0, 0, 3345, 6126, 10386, 1719]\nBinarizing and Patching Annotated F018a05.tif\nP [0, 0, 9585, 1809, 3303, 0, 0, 0, 3984, 18549, 15933, 0, 0, 0, 4317, 11316, 2832, 0, 0, 0, 6504, 8817, 4524, 0]\nBinarizing and Patching Annotated F034a08.tif\nP [7488, 10203, 3816, 3072, 1791, 4140, 9282, 13563, 20304, 12375, 12627, 10023, 5283, 12942, 20658, 25302, 28494, 31425, 2523, 6993, 5976, 31875, 17454, 27225]\nBinarizing and Patching Annotated F018a09.tif\nP [20046, 26628, 32145, 10509, 30309, 8544, 26847, 23130, 23490, 9606, 28032, 26175, 19524, 18363, 19161, 30285, 30153, 25905, 0, 0, 3351, 9393, 20301, 14601]\nBinarizing and Patching Annotated F006a10.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3774, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F052a10.tif\nP [0, 0, 4899, 3438, 72, 10620, 0, 0, 543, 5334, 2607, 15018, 0, 0, 0, 4161, 13788, 22146, 1869, 4884, 429, 1005, 12966, 18084]\nBinarizing and Patching Annotated F017a10.tif\nP [3444, 3477, 0, 10095, 84, 0, 0, 0, 0, 6843, 8874, 756, 0, 0, 3192, 7728, 8865, 7119, 5310, 8934, 4677, 1917, 2523, 3534]\nBinarizing and Patching Annotated F034a10.tif\nP [0, 3900, 8595, 0, 0, 0, 6519, 31299, 28404, 22947, 7926, 0, 2562, 2898, 19440, 25095, 17556, 432, 444, 5217, 27075, 15636, 25722, 6900]\nBinarizing and Patching Annotated F013a03.tif\nP [102, 5109, 0, 4533, 57, 0, 1245, 4935, 0, 1668, 4560, 0, 7302, 0, 276, 8625, 6477, 432, 333, 0, 702, 5889, 5358, 1215]\nBinarizing and Patching Annotated F056a10.tif\nP [1863, 0, 3156, 2508, 1371, 846, 4485, 1473, 1311, 30, 0, 0, 3816, 1830, 1917, 48, 0, 0, 528, 5817, 4221, 4614, 4026, 4818]\nBinarizing and Patching Annotated F033a20.tif\nP [11388, 18255, 5100, 624, 5502, 4257, 7779, 17991, 12018, 4689, 5517, 4422, 8808, 13860, 3837, 9219, 13281, 0, 6036, 8679, 3474, 7113, 16548, 9816]\nBinarizing and Patching Annotated F018a03.tif\nP [6156, 7479, 309, 2085, 11019, 3822, 1482, 27171, 9228, 6672, 0, 0, 0, 1203, 4404, 10401, 0, 0, 24, 4191, 4530, 1266, 0, 0]\nBinarizing and Patching Annotated F007a10.tif\nP [0, 12324, 13662, 6288, 0, 0, 6711, 17757, 10899, 11787, 480, 0, 0, 2283, 0, 1371, 132, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F011a01.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7278, 0, 0, 0, 0, 2058, 10920, 9195, 0, 0, 0, 4128, 4179, 7761]\nBinarizing and Patching Annotated F053a16.tif\nP [9924, 4755, 3036, 402, 0, 0, 20778, 19857, 12285, 5142, 0, 8289, 12483, 23928, 18036, 20700, 27990, 17061, 786, 13728, 7611, 21306, 29538, 15486]\nBinarizing and Patching Annotated F017a06.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 9420, 3036, 0, 0, 0, 1443, 4080, 4605, 3009, 12, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F053a13.tif\nP [5229, 471, 927, 4260, 255, 0, 13812, 11574, 3402, 3015, 0, 2946, 1242, 273, 8889, 8523, 3795, 375, 0, 0, 1149, 4701, 8730, 4119]\nBinarizing and Patching Annotated F017a04.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 9147, 3069, 0, 0, 0, 1095, 4188, 3693, 2889, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F030a03.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 414, 0, 0, 0, 2445, 4281, 4596, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F056a01.tif\nP [0, 0, 6108, 12099, 8484, 17223, 0, 6078, 37230, 29361, 18051, 13716, 0, 9024, 17223, 21324, 17697, 25011, 0, 14295, 4857, 10515, 19041, 17070]\nBinarizing and Patching Annotated F052a04.tif\nP [0, 4566, 786, 0, 0, 0, 6213, 0, 2322, 0, 0, 0, 15309, 0, 0, 4026, 2418, 30, 10632, 6720, 13815, 19191, 11271, 5244]\nBinarizing and Patching Annotated F011a02.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 2757, 0, 0, 0, 0, 9567, 19668, 9972, 0, 0, 3801, 13305, 12156, 24330, 0]\nBinarizing and Patching Annotated F056a05.tif\nP [11508, 9177, 0, 0, 1089, 0, 19836, 11217, 0, 7362, 6897, 1104, 8334, 3057, 3855, 7737, 2184, 0, 7416, 17286, 11070, 9111, 2697, 0]\nBinarizing and Patching Annotated F052a19.tif\nP [22239, 5010, 19455, 2817, 3321, 0, 6264, 1332, 1380, 8901, 2658, 4035, 741, 0, 0, 3060, 31806, 6855, 0, 0, 0, 4182, 16818, 6663]\nBinarizing and Patching Annotated F007a07.tif\nP [7329, 7146, 8241, 24270, 25467, 10347, 5946, 14760, 23505, 17397, 26439, 13176, 16695, 22005, 13731, 15888, 17085, 32580, 24858, 24558, 7353, 15969, 28818, 15621]\nBinarizing and Patching Annotated F007a03.tif\nP [0, 0, 3468, 23895, 20895, 15132, 0, 0, 15255, 26583, 20877, 24576, 0, 2775, 22845, 17220, 12117, 14511, 5298, 24528, 31098, 19911, 27942, 13698]\nBinarizing and Patching Annotated F007a22.tif\nP [17226, 17727, 17394, 25656, 19158, 732, 16827, 9540, 13053, 16566, 12000, 7338, 5733, 17154, 5754, 7371, 3318, 13935, 4743, 13677, 20988, 6480, 1275, 669]\nBinarizing and Patching Annotated F012a01.tif\nP [0, 0, 0, 0, 0, 0, 0, 2640, 22830, 28701, 1116, 0, 0, 1896, 25602, 29004, 0, 0, 0, 0, 225, 3249, 0, 0]\nBinarizing and Patching Annotated F006a06.tif\nP [0, 0, 0, 0, 309, 0, 0, 0, 0, 0, 5703, 0, 0, 0, 0, 0, 432, 1287, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F030a12.tif\nP [0, 6111, 8385, 5742, 8991, 1890, 1905, 4869, 3579, 6345, 21114, 6150, 4590, 6465, 354, 3855, 14355, 0, 777, 699, 111, 0, 0, 0]\nBinarizing and Patching Annotated F033a24.tif\nP [9201, 17901, 21591, 16878, 17040, 9849, 12318, 31443, 26811, 33273, 21477, 10677, 33579, 13704, 16905, 17346, 35874, 16245, 23532, 22188, 20730, 24654, 34734, 9057]\nBinarizing and Patching Annotated F013a12.tif\nP [0, 0, 0, 357, 558, 0, 0, 0, 0, 0, 3123, 474, 0, 0, 0, 2496, 4902, 261, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F033a11.tif\nP [27444, 13758, 33213, 18426, 618, 0, 20193, 14052, 13104, 24510, 13566, 1803, 12924, 7155, 2535, 21627, 3174, 4224, 16581, 10182, 4167, 0, 0, 306]\nBinarizing and Patching Annotated F009a02.tif\nP [0, 0, 1878, 5535, 0, 0, 0, 0, 1554, 3243, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F056a12.tif\nP [11283, 9450, 12504, 15672, 19428, 35310, 3048, 9324, 11964, 18168, 18729, 13809, 6192, 7200, 9246, 14526, 7599, 2055, 7857, 7437, 15684, 6951, 918, 2715]\nBinarizing and Patching Annotated F033a01.tif\nP [312, 0, 0, 0, 0, 0, 3390, 0, 0, 0, 0, 0, 678, 0, 288, 3249, 0, 0, 0, 2154, 5058, 10902, 0, 0]\nBinarizing and Patching Annotated F007a13.tif\nP [14514, 11103, 13569, 27873, 15390, 14616, 11160, 24219, 28062, 20190, 11784, 14385, 4170, 4629, 6231, 0, 13110, 23724, 0, 0, 0, 0, 2376, 18414]\nBinarizing and Patching Annotated F056a11.tif\nP [210, 6576, 2676, 14517, 9093, 15849, 690, 0, 1959, 2010, 3258, 7680, 0, 4245, 3627, 14292, 13233, 16986, 1680, 480, 9762, 4509, 10248, 12144]\nBinarizing and Patching Annotated F053a17.tif\nP [0, 2076, 7236, 10845, 9213, 3723, 981, 22848, 9480, 5103, 0, 0, 11751, 7179, 0, 0, 0, 0, 8745, 16872, 0, 0, 0, 0]\nBinarizing and Patching Annotated F056a14.tif\nP [1575, 1188, 5289, 17535, 11346, 7491, 3591, 15933, 14853, 9342, 8127, 6795, 21483, 12102, 25986, 10140, 7020, 4836, 15258, 33600, 18264, 5403, 960, 144]\nBinarizing and Patching Annotated F030a15.tif\nP [0, 0, 0, 3057, 0, 0, 0, 1431, 1026, 6381, 426, 0, 1011, 8163, 2727, 4434, 0, 4371, 4911, 8694, 7311, 231, 4149, 231]\nBinarizing and Patching Annotated F033a18.tif\nP [0, 0, 4749, 10998, 12525, 7830, 0, 375, 10701, 22905, 23475, 16917, 0, 1245, 6297, 20616, 14772, 7857, 0, 4443, 10899, 7026, 14304, 14496]\nBinarizing and Patching Annotated F030a18.tif\nP [4098, 4527, 0, 14673, 2730, 8919, 0, 5757, 5784, 8472, 9270, 5952, 0, 10773, 15513, 2595, 0, 0, 0, 2112, 7146, 0, 0, 0]\nBinarizing and Patching Annotated F052a15.tif\nP [0, 0, 0, 0, 4407, 0, 0, 0, 0, 2961, 3723, 0, 0, 0, 2133, 12321, 9621, 0, 0, 0, 0, 0, 1653, 0]\nBinarizing and Patching Annotated F018a13.tif\nP [2112, 14031, 28557, 38841, 46170, 19497, 12864, 32412, 39729, 41538, 10851, 42426, 28482, 43974, 30096, 31077, 7680, 32610, 6093, 29112, 27867, 32664, 32964, 32985]\nBinarizing and Patching Annotated F056a07.tif\nP [0, 0, 0, 0, 0, 0, 0, 1905, 975, 0, 0, 0, 7569, 25773, 11163, 0, 3774, 381, 10227, 12513, 5724, 0, 8538, 7440]\nBinarizing and Patching Annotated F006a09.tif\nP [0, 0, 2391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3648, 1287, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F058a04.tif\nP [0, 0, 63, 1395, 14022, 8523, 0, 3957, 159, 111, 138, 0, 0, 16878, 23325, 4116, 7881, 10143, 0, 5655, 20946, 17937, 24258, 28929]\nBinarizing and Patching Annotated F033a23.tif\nP [25146, 15711, 4545, 7512, 4926, 7284, 12780, 10392, 6453, 14367, 0, 10113, 6252, 4392, 5169, 17997, 8070, 5148, 17646, 12885, 10788, 14688, 11613, 2418]\nBinarizing and Patching Annotated F033a17.tif\nP [0, 0, 3732, 9855, 10242, 6675, 0, 174, 5985, 15471, 20811, 13233, 0, 477, 4704, 14907, 10137, 5367, 0, 1080, 8160, 6366, 5508, 4686]\nBinarizing and Patching Annotated F017a08.tif\nP [2931, 2829, 0, 7869, 129, 0, 0, 0, 0, 5385, 7524, 525, 0, 0, 2019, 8556, 7737, 6279, 3066, 7959, 3897, 1956, 6414, 2874]\nBinarizing and Patching Annotated F013a01.tif\nP [0, 0, 0, 5673, 3153, 0, 1965, 0, 0, 384, 4536, 0, 9540, 5706, 0, 0, 1398, 0, 5289, 2322, 0, 0, 1995, 5076]\nBinarizing and Patching Annotated F007a16.tif\nP [22293, 21219, 21402, 0, 2913, 3723, 16476, 8349, 1014, 3072, 0, 9894, 14484, 18141, 6540, 15786, 9453, 12048, 5967, 9426, 3744, 534, 7887, 1095]\nBinarizing and Patching Annotated F007a01.tif\nP [0, 0, 4032, 28200, 11754, 12675, 0, 0, 16239, 25122, 17337, 18657, 0, 1758, 14073, 19185, 19335, 19548, 3744, 18663, 23595, 22464, 30309, 11976]\nBinarizing and Patching Annotated F048a01.tif\nP [1197, 912, 48, 4008, 3231, 6, 915, 2664, 6, 2361, 1065, 0, 177, 1848, 444, 6795, 0, 0, 264, 18, 3150, 8445, 0, 0]\nBinarizing and Patching Annotated F017a05.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 10926, 3909, 0, 0, 0, 1098, 3897, 4335, 3168, 204, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F033a28.tif\nP [0, 0, 4413, 22641, 7890, 0, 0, 0, 1986, 18051, 7476, 0, 0, 1707, 7872, 6804, 27984, 3234, 1806, 22455, 13419, 20685, 24456, 18306]\nBinarizing and Patching Annotated F030a01.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 2844, 0, 0, 0, 0, 0, 579, 2856, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F052a02.tif\nP [0, 5292, 2514, 0, 0, 0, 5853, 0, 1803, 0, 0, 0, 14118, 0, 0, 1281, 0, 54, 10419, 5970, 13443, 16632, 12207, 7695]\nBinarizing and Patching Annotated F033a03.tif\nP [0, 0, 0, 0, 0, 0, 0, 8151, 3606, 0, 0, 0, 0, 4248, 5139, 4887, 219, 0, 0, 0, 0, 1725, 0, 0]\nBinarizing and Patching Annotated F052a16.tif\nP [0, 0, 0, 0, 1668, 0, 0, 0, 0, 3261, 3474, 0, 1989, 0, 7419, 15084, 11319, 0, 13419, 129, 0, 0, 1551, 0]\nBinarizing and Patching Annotated F021a04.tif\nP [0, 0, 1059, 1581, 0, 0, 0, 3027, 3594, 0, 0, 0, 0, 0, 6810, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F021a03.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 27, 1272, 0, 0, 0, 1890, 1911, 1026, 0, 0, 0, 2958, 8187, 0, 0]\nBinarizing and Patching Annotated F052a01.tif\nP [0, 0, 0, 3645, 3585, 7428, 2334, 3387, 6747, 7749, 2439, 9450, 5235, 3687, 7767, 5544, 2835, 4530, 10146, 8826, 12357, 8193, 0, 4245]\nBinarizing and Patching Annotated F053a11.tif\nP [0, 0, 5265, 1029, 7737, 6561, 0, 1500, 8739, 11991, 7416, 3855, 213, 24507, 10872, 4680, 459, 0, 9891, 10146, 4701, 3075, 0, 0]\nBinarizing and Patching Annotated F012a06.tif\nP [0, 0, 0, 0, 3435, 0, 0, 1590, 840, 0, 8031, 3570, 0, 3111, 6159, 3039, 207, 6717, 615, 6354, 12885, 4104, 0, 3498]\nBinarizing and Patching Annotated F013a05.tif\nP [5052, 0, 4977, 1533, 1212, 4137, 0, 3885, 2397, 0, 7338, 2901, 6372, 2772, 0, 0, 5952, 5688, 0, 4170, 0, 1443, 6081, 1581]\nBinarizing and Patching Annotated F053a10.tif\nP [510, 8391, 11562, 8634, 0, 0, 0, 2739, 6195, 6342, 0, 15, 6, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F006a05.tif\nP [0, 0, 0, 0, 945, 0, 0, 0, 0, 0, 5523, 30, 0, 0, 0, 0, 150, 1404, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F034a07.tif\nP [2226, 5028, 759, 9, 0, 0, 8487, 3147, 12129, 22767, 9606, 597, 20043, 27735, 25020, 11919, 14886, 20727, 26676, 28188, 40362, 20427, 23463, 8010]\nBinarizing and Patching Annotated F034a09.tif\nP [1923, 21765, 13929, 19650, 4812, 10500, 15969, 29871, 16524, 27960, 19017, 15801, 10419, 24057, 18483, 19032, 17229, 27636, 37551, 21753, 5502, 15819, 16044, 22398]\nBinarizing and Patching Annotated F007a04.tif\nP [8868, 13764, 12549, 23391, 14958, 7323, 2973, 23208, 12147, 20313, 12021, 0, 11565, 13698, 13746, 23901, 23265, 4521, 18582, 13848, 14688, 18990, 17229, 4053]\nBinarizing and Patching Annotated F034a05.tif\nP [17313, 27081, 26361, 9072, 20595, 34449, 6885, 17910, 18096, 18912, 32817, 53664, 2061, 25251, 32019, 24888, 8910, 20949, 0, 936, 27327, 24267, 9585, 552]\nBinarizing and Patching Annotated F021a05.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 894, 1452, 0, 0, 0, 5991, 5994, 246, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F052a08.tif\nP [3225, 12570, 8751, 6891, 3564, 14892, 0, 11958, 5676, 4257, 11526, 17046, 4245, 2712, 0, 10947, 17481, 11901, 0, 6192, 2286, 16200, 27435, 16821]\nBinarizing and Patching Annotated F033a19.tif\nP [0, 9492, 19650, 7950, 14379, 12864, 7509, 11526, 11643, 13398, 18267, 17904, 11889, 8634, 9459, 4467, 10257, 16653, 4902, 4047, 9303, 4398, 11712, 19368]\nBinarizing and Patching Annotated F030a10.tif\nP [0, 5991, 2445, 0, 2142, 3096, 0, 4812, 2274, 0, 0, 0, 0, 4359, 0, 0, 0, 0, 0, 318, 0, 1710, 0, 0]\nBinarizing and Patching Annotated F056a03.tif\nP [0, 0, 5535, 10155, 11889, 17091, 0, 0, 3096, 4959, 12438, 25887, 0, 753, 14433, 13338, 18720, 26079, 0, 0, 10848, 12870, 7611, 11427]\nBinarizing and Patching Annotated F009a03.tif\nP [0, 0, 1683, 5022, 0, 0, 0, 0, 1656, 3273, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F053a02.tif\nP [0, 792, 9003, 12069, 0, 3099, 891, 6096, 12948, 6726, 0, 0, 8277, 1113, 6, 3846, 0, 0, 5367, 1671, 0, 0, 0, 0]\nBinarizing and Patching Annotated F033a13.tif\nP [0, 765, 2637, 10068, 10095, 15735, 2238, 21678, 7272, 12504, 18183, 9543, 4812, 25980, 24444, 21624, 17646, 9828, 0, 2004, 25371, 18762, 12690, 28428]\nBinarizing and Patching Annotated F033a27.tif\nP [0, 8745, 12102, 2523, 2106, 516, 5622, 21873, 12189, 19770, 5829, 12726, 5430, 17502, 16476, 17811, 12330, 11286, 6750, 11109, 14598, 2259, 20208, 14709]\nBinarizing and Patching Annotated F053a18.tif\nP [0, 3402, 7740, 5328, 8484, 7134, 696, 22122, 10935, 7380, 3762, 2331, 11160, 7752, 186, 3213, 0, 0, 8454, 19332, 6459, 8403, 0, 0]\nBinarizing and Patching Annotated F033a14.tif\nP [0, 1293, 4524, 10665, 10155, 16848, 3441, 27780, 7200, 12915, 21858, 10401, 7386, 36660, 37872, 27414, 20931, 18471, 0, 3072, 35406, 21651, 13710, 28002]\nBinarizing and Patching Annotated F005a02.tif\nP [7032, 10353, 7677, 3762, 5256, 0, 279, 3897, 13623, 2697, 1800, 0, 0, 3981, 5706, 7824, 90, 0, 7494, 4839, 4695, 1200, 0, 0]\nBinarizing and Patching Annotated F017a07.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 732, 0, 0, 0, 0, 0, 738, 6303, 0, 0, 0, 0, 0, 408, 0, 0]\nBinarizing and Patching Annotated F006a04.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 693, 3024, 0, 0, 0, 0, 0, 0, 54]\nBinarizing and Patching Annotated F030a11.tif\nP [0, 3930, 5580, 0, 0, 0, 672, 3933, 0, 0, 0, 0, 0, 879, 2472, 0, 0, 0, 0, 0, 3489, 0, 0, 0]\nBinarizing and Patching Annotated F053a14.tif\nP [3969, 486, 0, 0, 0, 21, 14517, 12771, 1836, 2394, 993, 0, 4368, 4656, 10629, 10272, 5691, 486, 0, 3, 2241, 4791, 9702, 5241]\nBinarizing and Patching Annotated F033a04.tif\nP [0, 0, 0, 0, 0, 0, 0, 4971, 2802, 0, 0, 0, 0, 2136, 4188, 2547, 0, 0, 0, 0, 0, 3039, 0, 0]\nBinarizing and Patching Annotated F017a02.tif\nP [0, 0, 1554, 2127, 0, 0, 0, 0, 0, 4287, 1776, 0, 0, 2964, 0, 0, 435, 0, 0, 3048, 0, 0, 0, 0]\nBinarizing and Patching Annotated F056a17.tif\nP [0, 0, 657, 13389, 9072, 39, 0, 0, 1320, 12393, 11400, 420, 0, 258, 3936, 4455, 5769, 2919, 0, 690, 1455, 1029, 3732, 2658]\nBinarizing and Patching Annotated F030a02.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 705, 0, 0, 0, 0, 2334, 576, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F012a02.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 969, 6852, 2100, 0, 0, 0, 1548, 6156, 4836, 312, 0, 0, 0, 1569, 4377, 954]\nBinarizing and Patching Annotated F052a20.tif\nP [21246, 4503, 19908, 2940, 645, 0, 6033, 0, 1494, 8589, 2658, 3525, 819, 0, 0, 2598, 30816, 6660, 0, 0, 3, 4356, 17652, 6897]\nBinarizing and Patching Annotated F056a06.tif\nP [972, 0, 3870, 753, 0, 0, 8016, 9237, 23376, 7803, 0, 4617, 5991, 14001, 7494, 4521, 777, 9420, 15393, 15288, 6678, 14808, 11517, 36]\nBinarizing and Patching Annotated F012a03.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 3012, 0, 0, 0, 0, 75, 10461, 2451, 0, 0, 0, 0, 975, 4530, 0]\nBinarizing and Patching Annotated F050a02.tif\nP [465, 21, 0, 0, 0, 0, 21150, 3546, 0, 0, 0, 0, 11700, 12078, 5988, 0, 9, 0, 1848, 3894, 6336, 0, 0, 0]\nBinarizing and Patching Annotated F010a04.tif\nP [0, 7134, 390, 0, 6510, 7539, 0, 450, 7584, 13389, 8532, 2607, 5220, 0, 321, 13041, 7545, 5589, 8934, 9357, 9807, 3705, 0, 0]\nBinarizing and Patching Annotated F007a02.tif\nP [0, 0, 5433, 35763, 24060, 16665, 0, 0, 18495, 29430, 24348, 28278, 0, 1236, 22800, 20100, 19851, 23046, 3432, 24030, 27468, 21318, 24957, 14043]\nBinarizing and Patching Annotated F048a02.tif\nP [153, 570, 1053, 7638, 795, 555, 42, 0, 0, 9, 3471, 1350, 3, 0, 0, 0, 3624, 105, 0, 0, 0, 3696, 1296, 4509]\nBinarizing and Patching Annotated F058a02.tif\nP [10200, 7614, 6873, 0, 0, 0, 519, 570, 3729, 4605, 1227, 0, 0, 726, 2829, 15408, 25038, 28371, 0, 0, 1683, 4737, 23934, 12558]\nBinarizing and Patching Annotated F006a02.tif\nP [0, 0, 1299, 0, 0, 0, 0, 7179, 6075, 0, 0, 0, 0, 14451, 5505, 0, 0, 0, 0, 3342, 1224, 0, 0, 0]\nBinarizing and Patching Annotated F018a01.tif\nP [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5499, 0, 0, 0, 4602, 7506, 20985, 12870, 0, 0]\nBinarizing and Patching Annotated F053a06.tif\nP [11757, 9123, 3810, 0, 0, 0, 3312, 4218, 0, 0, 0, 0, 129, 342, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F013a06.tif\nP [0, 0, 0, 4566, 3927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 576, 3828, 378, 0, 0, 0, 6012, 0, 0]\nBinarizing and Patching Annotated F052a14.tif\nP [11862, 16089, 14568, 1845, 0, 0, 18525, 16197, 9315, 1539, 0, 756, 5316, 825, 4596, 0, 0, 4155, 0, 0, 0, 0, 1098, 2667]\nBinarizing and Patching Annotated F013a13.tif\nP [0, 366, 4683, 0, 0, 0, 0, 15039, 26910, 8172, 0, 0, 561, 21678, 25551, 30273, 9942, 0, 1503, 25260, 39645, 36861, 8370, 0]\nBinarizing and Patching Annotated F057a01.tif\nP [0, 0, 0, 0, 0, 0, 0, 3321, 252, 0, 0, 0, 414, 5070, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nBinarizing and Patching Annotated F053a07.tif\nP [15, 0, 813, 6969, 570, 1533, 0, 6489, 4455, 8604, 2055, 3687, 939, 3789, 9906, 6018, 0, 0, 2169, 10026, 10272, 3345, 0, 0]\nBinarizing and Patching Annotated F056a04.tif\nP [15108, 12981, 28764, 21171, 753, 0, 3801, 14070, 12231, 15282, 11211, 0, 10539, 24903, 24339, 12909, 7419, 0, 17481, 11604, 15225, 5409, 0, 0]\nBinarizing and Patching Annotated F056a02.tif\nP [15252, 13029, 3180, 0, 0, 0, 29838, 15396, 4332, 0, 0, 0, 6345, 2340, 5526, 0, 0, 0, 6867, 0, 0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{"_kg_hide-output":false}},{"cell_type":"code","source":"seed = 1\npatch_dir = os.path.abspath('/kaggle/working/patches/')\npatch_annot_dir = os.path.abspath('/kaggle/working/patches_annot/')\nfeat_dir = os.path.abspath('/kaggle/working/features/')","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:49:55.616981Z","iopub.execute_input":"2023-01-03T11:49:55.617387Z","iopub.status.idle":"2023-01-03T11:49:55.623570Z","shell.execute_reply.started":"2023-01-03T11:49:55.617354Z","shell.execute_reply":"2023-01-03T11:49:55.622237Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import os\nimport yaml\nimport argparse\n\n# import h5py\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.np_utils import to_categorical\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n\nfrom tensorflow.keras.utils import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom keras.callbacks import ModelCheckpoint\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2023-01-03T11:49:55.797258Z","iopub.execute_input":"2023-01-03T11:49:55.797550Z","iopub.status.idle":"2023-01-03T11:49:55.804305Z","shell.execute_reply.started":"2023-01-03T11:49:55.797523Z","shell.execute_reply":"2023-01-03T11:49:55.803241Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# !mkdir data\n# F = os.path.abspath('/kaggle/input/fungal-10x/')\n# !cp -r $F data\n# !ls data\n\n# !mkdir data/train\n# !mkdir data/train/fungal\n# !mv data/F* data/train/fungal/\n# !mkdir data/train/nonfungal\n# !mv data/N* data/train/nonfungal/\n\n# !ls data/train/fungal | wc -l\n# !ls data/train/nonfungal | wc -l","metadata":{"execution":{"iopub.status.busy":"2023-01-03T12:23:14.011936Z","iopub.execute_input":"2023-01-03T12:23:14.012415Z","iopub.status.idle":"2023-01-03T12:23:14.017976Z","shell.execute_reply.started":"2023-01-03T12:23:14.012375Z","shell.execute_reply":"2023-01-03T12:23:14.016877Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"epochs = 20\n\ntrain_ds, validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    os.path.abspath(\"/kaggle/working/data/train/fungal\"),\n    labels=\"inferred\",\n    batch_size=32,\n    image_size=(256, 256),\n    seed=seed\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T12:25:01.306926Z","iopub.execute_input":"2023-01-03T12:25:01.307314Z","iopub.status.idle":"2023-01-03T12:25:01.434997Z","shell.execute_reply.started":"2023-01-03T12:25:01.307280Z","shell.execute_reply":"2023-01-03T12:25:01.433649Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Found 0 files belonging to 0 classes.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2654750257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m       image_paths, labels, validation_split, subset)\n\u001b[1;32m    206\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No images found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   dataset = paths_and_labels_to_dataset(\n","\u001b[0;31mValueError\u001b[0m: No images found."],"ename":"ValueError","evalue":"No images found.","output_type":"error"}]},{"cell_type":"code","source":"# Create feat_dir if not exists.\nif not os.path.isdir(feat_dir):\n    os.mkdir(feat_dir)\n\ndata_augmentation = tf.keras.Sequential([\n  keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n  keras.layers.RandomRotation(0.2),\n])\n    \n# Loading ResNet50 wit imagenet weights, include_top means that we loading model without last fully connected layers\nbase_model  = ResNet50(weights = 'imagenet', include_top = False)\nbase_model.trainable = False  # Freeze base_model\n\n# Create new model on top\ninputs = keras.Input(shape=(256, 256, 3))\nx = data_augmentation(inputs)  # Apply random data augmentation\n\n# The base model contains batchnorm layers. We want to keep them in inference mode\n# when we unfreeze the base model for fine-tuning, so we make sure that the\n# base_model is running in inference mode here.\nx = base_model(x, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dropout(0.25)(x)  # Regularize with dropout\noutputs = keras.layers.Dense(1, activation=\"relu\")(x)\nmodel = keras.Model(inputs, outputs)\n\nmodel.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-01-03T08:55:24.276341Z","iopub.execute_input":"2023-01-03T08:55:24.277110Z","iopub.status.idle":"2023-01-03T08:55:26.203946Z","shell.execute_reply.started":"2023-01-03T08:55:24.277073Z","shell.execute_reply":"2023-01-03T08:55:26.202912Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_15 (InputLayer)        [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nsequential_3 (Sequential)    (None, 256, 256, 3)       0         \n_________________________________________________________________\nresnet50 (Functional)        (None, None, None, 2048)  23587712  \n_________________________________________________________________\nglobal_average_pooling2d_3 ( (None, 2048)              0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 2049      \n=================================================================\nTotal params: 23,589,761\nTrainable params: 2,049\nNon-trainable params: 23,587,712\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n\nepochs = 20\nmodel.fit(train_ds, epochs=epochs, validation_data=validation_ds)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T08:55:26.205887Z","iopub.execute_input":"2023-01-03T08:55:26.206527Z","iopub.status.idle":"2023-01-03T08:55:26.242696Z","shell.execute_reply.started":"2023-01-03T08:55:26.206489Z","shell.execute_reply":"2023-01-03T08:55:26.241269Z"},"trusted":true},"execution_count":30,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/56532482.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"],"ename":"NameError","evalue":"name 'train_ds' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# patch_folders = [os.path.join(patch_dir, folder) for folder in sorted(os.listdir(patch_dir))]\n# patches_per_image = len(os.listdir(patch_folders[0]))\n# print(patches_per_image)\n\n# Create dataset from the image patches\nfor folder in sorted(os.listdir(patch_dir)):\n    filename = str(folder).split(\"/\")[-1]\n    filePath = os.path.join(feat_dir, filename)\n    # Run only if file doesn't already exist\n    if os.path.exists(filePath):\n        print(\"Skipping File:\", filename)\n        continue\n    print(\"Running on File:\", filename)\n\n    features = []\n    patch_folder = os.path.join(patch_dir, folder)\n    for patch_file in sorted(os.listdir(patch_folder)):\n        img_path = os.path.join(patch_folder, patch_file)\n\n        # Get coord in [x, y] format\n        coord = img_path.split(\"/\")\n        coord = coord[-1]\n        coord = coord.split(\".\")[-2]\n        coord = coord.split(\"_\")\n        coord = [int(coord[-2])/256, int(coord[-1])/256]\n\n        # Read image\n        orig = cv2.imread(img_path)\n\n        # Convert image to RGB from BGR (another way is to use \"image = image[:, :, ::-1]\" code)\n        orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n\n        # Resize image to 224x224 size\n        image = cv2.resize(orig, (224, 224)).reshape(-1, 224, 224, 3)\n\n        # We need to preprocess imageto fulfill ResNet50 requirements\n        image = preprocess_input(image)\n\n        # Extracting our features\n        feature = model.predict(image)\n\n        # Group the features\n        features.append(feature)\n    np.save(filePath, features)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-01-03T08:31:35.104233Z","iopub.status.idle":"2023-01-03T08:31:35.106709Z","shell.execute_reply.started":"2023-01-03T08:31:35.106424Z","shell.execute_reply":"2023-01-03T08:31:35.106452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset creator","metadata":{}},{"cell_type":"code","source":"filename = os.path.join('/kaggle/working/', 'fungal_vs_nonfungal.csv')\npatch_dir = os.path.abspath('/kaggle/working/patches/')\nfeat_dir = os.path.abspath('/kaggle/working/features/')\nannotated_dir = os.path.abspath('/kaggle/input/fungal-10x-annot/')","metadata":{"execution":{"iopub.status.busy":"2022-12-29T13:21:45.262774Z","iopub.execute_input":"2022-12-29T13:21:45.263667Z","iopub.status.idle":"2022-12-29T13:21:45.269362Z","shell.execute_reply.started":"2022-12-29T13:21:45.263618Z","shell.execute_reply":"2022-12-29T13:21:45.268164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(filename, 'w') as file:\n    file.write('case_id,slide_id,label' + '\\n')\n\n    patch_folders = [os.path.join(patch_dir, folder) for folder in sorted(os.listdir(patch_dir))]\n\n    for i, name in enumerate(patch_folders):\n        name = name.split(\"/\")[-1]\n        if name != feat_dir:\n            if name[0] == \"F\":\n                f_nf = \"fungal\"\n            elif name[0] == \"N\":\n                f_nf = \"nonfungal\"\n                annotated = True\n            else:\n                f_nf = \"unclassified\"\n\n            line = 'case_' + str(i) + ',' + name + ',' + f_nf\n            file.write('{}\\n'.format(line))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-29T13:21:52.775619Z","iopub.execute_input":"2022-12-29T13:21:52.776219Z","iopub.status.idle":"2022-12-29T13:21:52.787711Z","shell.execute_reply.started":"2022-12-29T13:21:52.776176Z","shell.execute_reply":"2022-12-29T13:21:52.786679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Splits","metadata":{}},{"cell_type":"code","source":"label_frac = 1.0\nseed = 1\nk = 5\nval_frac = 0.15\ntest_frac = 0.15\nannot_frac = 0.4\nannot_positive_frac = 1\n\ndataset_csv_file = os.path.join('/kaggle/working/', 'fungal_vs_nonfungal.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:29:04.723649Z","iopub.execute_input":"2022-12-30T10:29:04.724143Z","iopub.status.idle":"2022-12-30T10:29:04.731706Z","shell.execute_reply.started":"2022-12-30T10:29:04.724106Z","shell.execute_reply":"2022-12-30T10:29:04.730422Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import os\nimport yaml\nimport random\nimport argparse\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:29:05.504688Z","iopub.execute_input":"2022-12-30T10:29:05.505444Z","iopub.status.idle":"2022-12-30T10:29:05.510469Z","shell.execute_reply.started":"2022-12-30T10:29:05.505405Z","shell.execute_reply":"2022-12-30T10:29:05.509475Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function, division\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport math\nimport re\nimport pdb\nimport pickle\nimport random\nfrom scipy import stats\n\nfrom torch.utils.data import Dataset\nimport h5py","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:29:06.760851Z","iopub.execute_input":"2022-12-30T10:29:06.761297Z","iopub.status.idle":"2022-12-30T10:29:06.971494Z","shell.execute_reply.started":"2022-12-30T10:29:06.761261Z","shell.execute_reply":"2022-12-30T10:29:06.970555Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def generate_split(cls_ids, val_num, test_num, samples, n_splits = 5,\n    seed = 7, label_frac = 1.0, custom_test_ids = None):\n    indices = np.arange(samples).astype(int)\n\n    if custom_test_ids is not None:\n        indices = np.setdiff1d(indices, custom_test_ids)\n\n    np.random.seed(seed)\n    for i in range(n_splits):\n        all_val_ids = []\n        all_test_ids = []\n        sampled_train_ids = []\n\n        if custom_test_ids is not None: # pre-built test split, do not need to sample\n            all_test_ids.extend(custom_test_ids)\n\n        for c in range(len(val_num)):\n            possible_indices = np.intersect1d(cls_ids[c], indices) #all indices of this class\n            val_ids = np.random.choice(possible_indices, val_num[c], replace = False) # validation ids\n\n            remaining_ids = np.setdiff1d(possible_indices, val_ids) #indices of this class left after validation\n            all_val_ids.extend(val_ids)\n\n            if custom_test_ids is None: # sample test split\n\n                test_ids = np.random.choice(remaining_ids, test_num[c], replace = False)\n                remaining_ids = np.setdiff1d(remaining_ids, test_ids)\n                all_test_ids.extend(test_ids)\n\n            if label_frac == 1:\n                sampled_train_ids.extend(remaining_ids)\n\n            else:\n                sample_num  = math.ceil(len(remaining_ids) * label_frac)\n                slice_ids = np.arange(sample_num)\n                sampled_train_ids.extend(remaining_ids[slice_ids])\n\n        yield sampled_train_ids, all_val_ids, all_test_ids\n\n\ndef nth(iterator, n, default=None):\n    if n is None:\n        return collections.deque(iterator, maxlen=0)\n    else:\n        return next(islice(iterator,n, None), default)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:29:08.150556Z","iopub.execute_input":"2022-12-30T10:29:08.151011Z","iopub.status.idle":"2022-12-30T10:29:08.164346Z","shell.execute_reply.started":"2022-12-30T10:29:08.150974Z","shell.execute_reply":"2022-12-30T10:29:08.163162Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def save_splits(split_datasets, column_keys, filename, annot_frac=None, annot_positive_frac=None, boolean_style=False, annot_create=True):\n    print(split_datasets)\n    splits = [split_datasets[i].slide_data['slide_id'] for i in range(len(split_datasets))]\n\n    if annot_create:\n        # Add annot column # Only for 2 classes\n        train_set = split_datasets[0]\n        train_set_list = []\n        annot_set = []\n        positive_list = []\n        negative_list = []\n\n        for ids in train_set.slide_cls_ids[0]:\n            negative_list.append(str(train_set.slide_data['slide_id'][ids]))\n\n        for ids in train_set.slide_cls_ids[1]:\n            positive_list.append(str(train_set.slide_data['slide_id'][ids]))\n\n        train_set_list.extend(negative_list)\n        train_set_list.extend(positive_list)\n\n        train_set_annot = np.round(len(train_set_list) * annot_frac)\n        neg_annot_num = np.round(train_set_annot * (1-annot_positive_frac)).astype(int)\n        pos_annot_num = np.round(train_set_annot * annot_positive_frac).astype(int)\n\n        neg_annot_set = random.sample(negative_list, neg_annot_num)\n        pos_annot_set = random.sample(positive_list, pos_annot_num)\n\n        annot_set.extend(neg_annot_set)\n        annot_set.extend(pos_annot_set)\n\n    #     print(\"annot_set\", annot_set)\n\n        true_annot_set = [False]*len(train_set_list)\n        for idx in range(len(true_annot_set)):\n            if train_set_list[idx] in annot_set:\n                true_annot_set[idx] = True\n    #     print(\"true_annot_set\", true_annot_set)\n        true_annot_set = pd.DataFrame(true_annot_set)\n    #     print(\"splits\", splits)\n    #     print(\"true_annot_set\", true_annot_set)\n        splits.insert(1, true_annot_set)\n\n    if not boolean_style:\n        df = pd.concat(splits, ignore_index=True, axis=1)\n        df.columns = column_keys\n    else:\n        df = pd.concat(splits, ignore_index = True, axis=0)\n        index = df.values.tolist()\n        one_hot = np.eye(len(split_datasets)).astype(bool)\n        bool_array = np.repeat(one_hot, [len(dset) for dset in split_datasets], axis=0)\n        df = pd.DataFrame(bool_array, index=index, columns = ['train', 'annot', 'val', 'test'])\n\n    print(split_datasets[0].slide_data)\n    df.to_csv(filename)\n    print()\n\nclass Generic_WSI_Classification_Dataset(Dataset):\n    def __init__(self,\n        csv_path = 'dataset_csv/ccrcc_clean.csv',\n        shuffle = False,\n        seed = 7,\n        print_info = True,\n        label_dict = {},\n        filter_dict = {},\n        ignore=[],\n        patient_strat=False,\n        label_col = None,\n        patient_voting = 'max',\n        results_dir = None\n        ):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            shuffle (boolean): Whether to shuffle\n            seed (int): random seed for shuffling the data\n            print_info (boolean): Whether to print a summary of the dataset\n            label_dict (dict): Dictionary with key, value pairs for converting str labels to int\n            ignore (list): List containing class labels to ignore\n        \"\"\"\n        self.label_dict = label_dict\n        self.num_classes = len(set(self.label_dict.values()))\n        self.seed = seed\n        self.print_info = print_info\n        self.patient_strat = patient_strat\n        self.train_ids, self.val_ids, self.test_ids  = (None, None, None)\n        self.data_dir = None\n        self.annot_dir = None\n        if not label_col:\n            label_col = 'label'\n        self.label_col = label_col\n\n        slide_data = pd.read_csv(csv_path)\n        slide_data = self.filter_df(slide_data, filter_dict)\n        slide_data = self.df_prep(slide_data, self.label_dict, ignore, self.label_col)\n        print(slide_data)\n\n        ###shuffle data\n        if shuffle:\n            np.random.seed(seed)\n            np.random.shuffle(slide_data)\n\n        self.slide_data = slide_data\n        if results_dir:\n            slide_data.to_csv(os.path.join(results_dir, 'dataset_csv.csv'))\n\n        self.patient_data_prep(patient_voting)\n        self.cls_ids_prep()\n\n        if print_info:\n            self.summarize()\n\n    def cls_ids_prep(self):\n        # store ids corresponding each class at the patient or case level\n        self.patient_cls_ids = [[] for i in range(self.num_classes)]\n        for i in range(self.num_classes):\n            self.patient_cls_ids[i] = np.where(self.patient_data['label'] == i)[0]\n\n        # store ids corresponding each class at the slide level\n        self.slide_cls_ids = [[] for i in range(self.num_classes)]\n        for i in range(self.num_classes):\n            self.slide_cls_ids[i] = np.where(self.slide_data['label'] == i)[0]\n\n    def patient_data_prep(self, patient_voting='max'):\n        patients = np.unique(np.array(self.slide_data['case_id'])) # get unique patients\n        patient_labels = []\n\n        for p in patients:\n            locations = self.slide_data[self.slide_data['case_id'] == p].index.tolist()\n            assert len(locations) > 0\n            label = self.slide_data['label'][locations].values\n            if patient_voting == 'max':\n                label = label.max() # get patient label (MIL convention)\n            elif patient_voting == 'maj':\n                label = stats.mode(label)[0]\n            else:\n                raise NotImplementedError\n            patient_labels.append(label)\n\n        self.patient_data = {'case_id':patients, 'label':np.array(patient_labels)}\n\n    @staticmethod\n    def df_prep(data, label_dict, ignore, label_col):\n        if label_col != 'label':\n            data['label'] = data[label_col].copy()\n\n        mask = data['label'].isin(ignore)\n        data = data[~mask]\n        data.reset_index(drop=True, inplace=True)\n        for i in data.index:\n            key = data.loc[i, 'label']\n            data.at[i, 'label'] = label_dict[key]\n\n        return data\n\n    def filter_df(self, df, filter_dict={}):\n        if len(filter_dict) > 0:\n            filter_mask = np.full(len(df), True, bool)\n            # assert 'label' not in filter_dict.keys()\n            for key, val in filter_dict.items():\n                mask = df[key].isin(val)\n                filter_mask = np.logical_and(filter_mask, mask)\n            df = df[filter_mask]\n        return df\n\n    def __len__(self):\n        if self.patient_strat:\n            return len(self.patient_data['case_id'])\n\n        else:\n            return len(self.slide_data)\n\n    def summarize(self):\n        print(\"label column: {}\".format(self.label_col))\n        print(\"label dictionary: {}\".format(self.label_dict))\n        print(\"number of classes: {}\".format(self.num_classes))\n        print(\"slide-level counts: \", '\\n', self.slide_data['label'].value_counts(sort = False))\n        for i in range(self.num_classes):\n            print('Patient-LVL; Number of samples registered in class %d: %d' % (i, self.patient_cls_ids[i].shape[0]))\n            print('Slide-LVL; Number of samples registered in class %d: %d' % (i, self.slide_cls_ids[i].shape[0]))\n\n    def create_splits(self, k = 3, val_num = (25, 25), test_num = (40, 40), label_frac = 1.0, custom_test_ids = None):\n        settings = {\n                    'n_splits' : k,\n                    'val_num' : val_num,\n                    'test_num': test_num,\n                    'label_frac': label_frac,\n                    'seed': self.seed,\n                    'custom_test_ids': custom_test_ids\n                    }\n\n        if self.patient_strat:\n            settings.update({'cls_ids' : self.patient_cls_ids, 'samples': len(self.patient_data['case_id'])})\n        else:\n            settings.update({'cls_ids' : self.slide_cls_ids, 'samples': len(self.slide_data)})\n\n        self.split_gen = generate_split(**settings)\n\n    def set_splits(self,start_from=None):\n        if start_from:\n            ids = nth(self.split_gen, start_from)\n\n        else:\n            ids = next(self.split_gen)\n\n        if self.patient_strat:\n            slide_ids = [[] for i in range(len(ids))]\n\n            for split in range(len(ids)):\n                for idx in ids[split]:\n                    case_id = self.patient_data['case_id'][idx]\n                    slide_indices = self.slide_data[self.slide_data['case_id'] == case_id].index.tolist()\n                    slide_ids[split].extend(slide_indices)\n\n            self.train_ids, self.val_ids, self.test_ids = slide_ids[0], slide_ids[1], slide_ids[2]\n\n        else:\n            self.train_ids, self.val_ids, self.test_ids = ids\n\n    def get_split_from_df(self, all_splits, split_key='train'):\n        split = all_splits[split_key]\n        split = split.dropna().reset_index(drop=True)\n\n        if len(split) > 0:\n            mask = self.slide_data['slide_id'].isin(split.tolist())\n            df_slice = self.slide_data[mask].reset_index(drop=True)\n            split = Generic_Split(df_slice, data_dir=self.data_dir, annot_dir=self.annot_dir, num_classes=self.num_classes)\n        else:\n            split = None\n\n        return split\n\n    def get_merged_split_from_df(self, all_splits, split_keys=['train']):\n        merged_split = []\n        for split_key in split_keys:\n            split = all_splits[split_key]\n            split = split.dropna().reset_index(drop=True).tolist()\n            merged_split.extend(split)\n\n        if len(split) > 0:\n            mask = self.slide_data['slide_id'].isin(merged_split)\n            df_slice = self.slide_data[mask].reset_index(drop=True)\n            split = Generic_Split(df_slice, data_dir=self.data_dir, num_classes=self.num_classes)\n        else:\n            split = None\n\n        return split\n\n    def get_overlap_split_from_df(self, all_splits, split_keys=['train', 'annot']):\n        train_split = all_splits['train']\n        annot_split = all_splits['annot']\n\n        if len(train_split) > 0:\n            mask = self.slide_data['slide_id'].isin(train_split)\n            df_slice = self.slide_data[mask].reset_index(drop=True)\n\n            mask = train_split.isin(df_slice['slide_id'].tolist())\n            df_slice['annot'] = annot_split[mask]\n\n            split = Generic_Split(df_slice, data_dir=self.data_dir, annot_dir=self.annot_dir, num_classes=self.num_classes)\n        else:\n            split = None\n\n        return split\n\n\n    def return_splits(self, from_id=True, csv_path=None):\n\n\n        if from_id:\n            if len(self.train_ids) > 0:\n                train_data = self.slide_data.loc[self.train_ids].reset_index(drop=True)\n                train_split = Generic_Split(train_data, annot_dir=self.annot_dir, data_dir=self.data_dir, num_classes=self.num_classes)\n\n            else:\n                train_split = None\n\n            if len(self.val_ids) > 0:\n                val_data = self.slide_data.loc[self.val_ids].reset_index(drop=True)\n                val_split = Generic_Split(val_data, data_dir=self.data_dir, num_classes=self.num_classes)\n\n            else:\n                val_split = None\n\n            if len(self.test_ids) > 0:\n                test_data = self.slide_data.loc[self.test_ids].reset_index(drop=True)\n                test_split = Generic_Split(test_data, data_dir=self.data_dir, num_classes=self.num_classes)\n\n            else:\n                test_split = None\n\n\n        else:\n            assert csv_path\n            all_splits = pd.read_csv(csv_path, dtype=self.slide_data['slide_id'].dtype)  # Without \"dtype=self.slide_data['slide_id'].dtype\", read_csv() will convert all-number columns to a numerical type. Even if we convert numerical columns back to objects later, we may lose zero-padding in the process; the columns must be correctly read in from the get-go. When we compare the individual train/val/test columns to self.slide_data['slide_id'] in the get_split_from_df() method, we cannot compare objects (strings) to numbers or even to incorrectly zero-padded objects/strings. An example of this breaking is shown in https://github.com/andrew-weisman/clam_analysis/tree/main/datatype_comparison_bug-2021-12-01.\n            train_split = self.get_overlap_split_from_df(all_splits, ['train', 'annot'])\n            val_split = self.get_overlap_split_from_df(all_splits, ['val', 'annot'])\n            test_split = self.get_overlap_split_from_df(all_splits, ['test', 'annot'])\n            # train_split = self.get_split_from_df(all_splits, 'train')\n            # val_split = self.get_split_from_df(all_splits, 'val')\n            # test_split = self.get_split_from_df(all_splits, 'test')\n\n        return train_split, val_split, test_split\n\n    def get_list(self, ids):\n        return self.slide_data['slide_id'][ids]\n\n    def getlabel(self, ids):\n        return self.slide_data['label'][ids]\n\n    def __getitem__(self, idx):\n        return None\n\n    def test_split_gen(self, return_descriptor=False):\n\n        if return_descriptor:\n            index = [list(self.label_dict.keys())[list(self.label_dict.values()).index(i)] for i in range(self.num_classes)]\n            columns = ['train', 'val', 'test']\n            df = pd.DataFrame(np.full((len(index), len(columns)), 0, dtype=np.int32), index= index,\n                            columns= columns)\n\n        count = len(self.train_ids)\n        print('\\nnumber of training samples: {}'.format(count))\n        labels = self.getlabel(self.train_ids)\n        unique, counts = np.unique(labels, return_counts=True)\n        for u in range(len(unique)):\n            print('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n            if return_descriptor:\n                df.loc[index[u], 'train'] = counts[u]\n\n        count = len(self.val_ids)\n        print('\\nnumber of val samples: {}'.format(count))\n        labels = self.getlabel(self.val_ids)\n        unique, counts = np.unique(labels, return_counts=True)\n        for u in range(len(unique)):\n            print('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n            if return_descriptor:\n                df.loc[index[u], 'val'] = counts[u]\n\n        count = len(self.test_ids)\n        print('\\nnumber of test samples: {}'.format(count))\n        labels = self.getlabel(self.test_ids)\n        unique, counts = np.unique(labels, return_counts=True)\n        for u in range(len(unique)):\n            print('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n            if return_descriptor:\n                df.loc[index[u], 'test'] = counts[u]\n\n        assert len(np.intersect1d(self.train_ids, self.test_ids)) == 0\n        assert len(np.intersect1d(self.train_ids, self.val_ids)) == 0\n        assert len(np.intersect1d(self.val_ids, self.test_ids)) == 0\n\n        if return_descriptor:\n            return df\n\n    def save_split(self, filename):\n        train_split = self.get_list(self.train_ids)\n        val_split = self.get_list(self.val_ids)\n        test_split = self.get_list(self.test_ids)\n        df_tr = pd.DataFrame({'train': train_split})\n        df_v = pd.DataFrame({'val': val_split})\n        df_t = pd.DataFrame({'test': test_split})\n        df = pd.concat([df_tr, df_v, df_t], axis=1)\n        df.to_csv(filename, index = False)\n\n\nclass Generic_MIL_Dataset(Generic_WSI_Classification_Dataset):\n    def __init__(self,\n        data_dir,\n        annot_dir=None,\n        patch_annot_dir=None,\n        **kwargs):\n\n        super(Generic_MIL_Dataset, self).__init__(**kwargs)\n        self.data_dir = data_dir\n        self.annot_dir = annot_dir\n        self.patch_annot_dir = patch_annot_dir\n        self.use_h5 = False\n\n    def load_from_h5(self, toggle):\n        self.use_h5 = toggle\n\n    def __getitem__(self, idx):\n        slide_id = self.slide_data['slide_id'][idx]\n        label = self.slide_data['label'][idx]\n        if self.slide_data['annot'][idx]:\n            bool_annot = bool(self.slide_data['annot'][idx])\n            if label == 1:\n                patch_annot_path = os.path.join(self.patch_annot_dir, slide_id, slide_id+'.pkl')\n                patch_annot = load_pkl(patch_annot_path)\n                patch_annot = patch_annot['bin_scores']\n            elif label == 0:\n                patch_annot = [False]*24\n\n        if type(self.data_dir) == dict:\n            source = self.slide_data['source'][idx]\n            data_dir = self.data_dir[source]\n        else:\n            data_dir = self.data_dir\n\n        if not self.use_h5:\n            if self.data_dir:\n                full_path = os.path.join(data_dir, '{}.pt'.format(slide_id))\n                features = torch.load(full_path)\n                return features, label, bool_annot, patch_annot\n                # return features, label\n\n            else:\n                # if bool_annot:\n                #     return slide_id, label, bool_annot, patch_annot\n                # else:\n                #     return slide_id, label, None, None\n                return slide_id, label\n\n        else:\n            full_path = os.path.join(data_dir,'h5_files','{}.h5'.format(slide_id))\n            with h5py.File(full_path,'r') as hdf5_file:\n                features = hdf5_file['features'][:]\n                coords = hdf5_file['coords'][:]\n\n            features = torch.from_numpy(features)\n            return features, label, coords\n\n\nclass Generic_Split(Generic_MIL_Dataset):\n    def __init__(self, slide_data, annot_dir=None, patch_annot_dir=None, data_dir=None, num_classes=2):\n        self.use_h5 = False\n        self.slide_data = slide_data\n        self.data_dir = data_dir\n        self.annot_dir = annot_dir\n        self.patch_annot_dir = patch_annot_dir,\n        self.num_classes = num_classes\n        self.slide_cls_ids = [[] for i in range(self.num_classes)]\n        for i in range(self.num_classes):\n            self.slide_cls_ids[i] = np.where(self.slide_data['label'] == i)[0]\n\n    def __len__(self):\n        return len(self.slide_data)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:41.037566Z","iopub.execute_input":"2022-12-30T10:46:41.038092Z","iopub.status.idle":"2022-12-30T10:46:41.159678Z","shell.execute_reply.started":"2022-12-30T10:46:41.038055Z","shell.execute_reply":"2022-12-30T10:46:41.158633Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"random.seed(seed)\n\n# task_1_fungal_vs_nonfungal\nn_classes=2\ndataset = Generic_WSI_Classification_Dataset(csv_path = dataset_csv_file,\n                        shuffle = False,\n                        seed = seed,\n                        print_info = True,\n                        label_dict = {'nonfungal':0, 'fungal':1},\n                        patient_strat=True,\n                        ignore=[])\n\nnum_slides_cls = np.array([len(cls_ids) for cls_ids in dataset.patient_cls_ids])\nval_num = np.round(num_slides_cls * val_frac).astype(int)\ntest_num = np.round(num_slides_cls * test_frac).astype(int)\n\nif label_frac > 0:\n    label_fracs = [label_frac]\nelse:\n    label_fracs = [0.1, 0.25, 0.5, 0.75, 1.0]\n\nfor lf in label_fracs:\n    split_dir = 'splits/fungal_vs_nonfungal' + '_{}'.format(int(lf * 100))\n    os.makedirs(split_dir, exist_ok=True)\n    dataset.create_splits(k = k, val_num = val_num, test_num = test_num, label_frac=lf)\n    for i in range(k):\n        dataset.set_splits()\n        descriptor_df = dataset.test_split_gen(return_descriptor=True)\n        splits = dataset.return_splits(from_id=True)\n\n        save_splits(splits, ['train', 'annot', 'val', 'test'], os.path.join(split_dir, 'splits_{}.csv'.format(i)), annot_frac=annot_frac, annot_positive_frac=annot_positive_frac)\n        # save_splits(splits, ['train', 'annot', 'val', 'test'], os.path.join(split_dir, 'splits_{}_bool.csv'.format(i)), boolean_style=True)\n        # descriptor_df.to_csv(os.path.join(split_dir, 'splits_{}_descriptor.csv'.format(i)))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-30T10:46:41.163609Z","iopub.execute_input":"2022-12-30T10:46:41.165339Z","iopub.status.idle":"2022-12-30T10:46:42.593559Z","shell.execute_reply.started":"2022-12-30T10:46:41.165253Z","shell.execute_reply":"2022-12-30T10:46:42.591829Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"      case_id  slide_id label\n0      case_0   F005a02     1\n1      case_1   F006a01     1\n2      case_2   F006a02     1\n3      case_3   F006a03     1\n4      case_4   F006a04     1\n..        ...       ...   ...\n418  case_418  N012a017     0\n419  case_419  N012a018     0\n420  case_420  N012a019     0\n421  case_421  N012a020     0\n422  case_422  N017a001     0\n\n[423 rows x 3 columns]\nlabel column: label\nlabel dictionary: {'nonfungal': 0, 'fungal': 1}\nnumber of classes: 2\nslide-level counts:  \n 1    208\n0    215\nName: label, dtype: int64\nPatient-LVL; Number of samples registered in class 0: 215\nSlide-LVL; Number of samples registered in class 0: 215\nPatient-LVL; Number of samples registered in class 1: 208\nSlide-LVL; Number of samples registered in class 1: 208\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7f68a6200890>, <__main__.Generic_Split object at 0x7f68a5806750>, <__main__.Generic_Split object at 0x7f68a580c510>)\n      case_id  slide_id label\n0    case_208  N001a001     0\n1    case_209  N001a002     0\n2    case_210  N003a001     0\n3    case_211  N003a002     0\n4    case_217  N004a005     0\n..        ...       ...   ...\n292   case_94   F030a06     1\n293   case_95   F030a07     1\n294   case_96   F030a08     1\n295   case_98   F030a10     1\n296   case_99   F030a11     1\n\n[297 rows x 3 columns]\n\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7f68b05efa50>, <__main__.Generic_Split object at 0x7f68a5806910>, <__main__.Generic_Split object at 0x7f68a57c7490>)\n      case_id  slide_id label\n0    case_208  N001a001     0\n1    case_210  N003a001     0\n2    case_211  N003a002     0\n3    case_212  N003a003     0\n4    case_215  N004a003     0\n..        ...       ...   ...\n292   case_94   F030a06     1\n293   case_95   F030a07     1\n294   case_96   F030a08     1\n295   case_97   F030a09     1\n296   case_99   F030a11     1\n\n[297 rows x 3 columns]\n\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7f693623cf50>, <__main__.Generic_Split object at 0x7f68b05f0d90>, <__main__.Generic_Split object at 0x7f68a57c7910>)\n      case_id  slide_id label\n0    case_208  N001a001     0\n1    case_209  N001a002     0\n2    case_212  N003a003     0\n3    case_213  N004a001     0\n4    case_215  N004a003     0\n..        ...       ...   ...\n292   case_93   F030a05     1\n293   case_95   F030a07     1\n294   case_96   F030a08     1\n295   case_97   F030a09     1\n296   case_98   F030a10     1\n\n[297 rows x 3 columns]\n\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7f68b05efa50>, <__main__.Generic_Split object at 0x7f68a5806850>, <__main__.Generic_Split object at 0x7f68a57bb4d0>)\n      case_id  slide_id label\n0    case_208  N001a001     0\n1    case_210  N003a001     0\n2    case_211  N003a002     0\n3    case_213  N004a001     0\n4    case_215  N004a003     0\n..        ...       ...   ...\n292   case_92   F030a04     1\n293   case_93   F030a05     1\n294   case_95   F030a07     1\n295   case_97   F030a09     1\n296   case_99   F030a11     1\n\n[297 rows x 3 columns]\n\n\nnumber of training samples: 297\nnumber of samples in cls 0: 151\nnumber of samples in cls 1: 146\n\nnumber of val samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n\nnumber of test samples: 63\nnumber of samples in cls 0: 32\nnumber of samples in cls 1: 31\n(<__main__.Generic_Split object at 0x7f693623cf50>, <__main__.Generic_Split object at 0x7f68a58069d0>, <__main__.Generic_Split object at 0x7f68a6216150>)\n      case_id  slide_id label\n0    case_209  N001a002     0\n1    case_210  N003a001     0\n2    case_211  N003a002     0\n3    case_213  N004a001     0\n4    case_214  N004a002     0\n..        ...       ...   ...\n292   case_94   F030a06     1\n293   case_95   F030a07     1\n294   case_96   F030a08     1\n295   case_97   F030a09     1\n296   case_98   F030a10     1\n\n[297 rows x 3 columns]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# View the splits\n# !cat splits/fungal_vs_nonfungal_100/splits_0.csv","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:42.596153Z","iopub.execute_input":"2022-12-30T10:46:42.596526Z","iopub.status.idle":"2022-12-30T10:46:42.601602Z","shell.execute_reply.started":"2022-12-30T10:46:42.596494Z","shell.execute_reply":"2022-12-30T10:46:42.600376Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"max_epochs = 200\nlr = 1e-4\nlabel_frac = 1.0\nreg = 1e-5\nseed = 1\nk = 5\nk_start = -1\nk_end = -1\ndataset_csv = os.path.join('/kaggle/working/fungal_vs_nonfungal.csv')\nresults_parent_dir = os.path.join('/kaggle/working/results/')\nsplit_dir = os.path.join('/kaggle/working/splits/fungal_vs_nonfungal_100/')\nlog_data = False\ntesting = False\nearly_stopping = False\nopt = 'adam'\ndrop_out = False\nbag_loss = 'ce'  # ['ce', 'svm']\nmodel_type = 'clam_sb'  # ['clam_sb', 'clam_mb', 'mil']\nweighted_sample = False\nmodel_size = 'small'\ntask = 'task_fungal_vs_nonfungal'\n\n### CLAM specific options\nno_inst_cluster = False\ninst_loss = None  # ['svm', 'ce', None]\nsubtyping = False\nbag_weight = 0.5\nB = 12\n\nexp_code = \"exp_01\"  # Experiment name\ndropout = True  # Whether to use dropout\npatch_dir = os.path.abspath('/kaggle/working/patches/')\ndest_dir = os.path.abspath('/kaggle/working/splits/')\nannot_dir = os.path.abspath('/kaggle/input/fungal-10x-annot/')\npatch_annot_dir = os.path.abspath('/kaggle/working/patch_annot/')\nfeat_dir = os.path.abspath('/kaggle/working/features/')\n\n### Alpha weight\nalpha_weight = False\nT1 = 50\nT2 = 150\naf = 1.0","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:42.602701Z","iopub.execute_input":"2022-12-30T10:46:42.603119Z","iopub.status.idle":"2022-12-30T10:46:42.625227Z","shell.execute_reply.started":"2022-12-30T10:46:42.603082Z","shell.execute_reply":"2022-12-30T10:46:42.623403Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade setuptools\n\n# !git clone https://github.com/oval-group/smooth-topk.git\n# !cd smooth-topk && python setup.py install","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:42.628136Z","iopub.execute_input":"2022-12-30T10:46:42.628645Z","iopub.status.idle":"2022-12-30T10:46:42.652914Z","shell.execute_reply.started":"2022-12-30T10:46:42.628607Z","shell.execute_reply":"2022-12-30T10:46:42.651182Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"import pdb\nimport os\nimport yaml\nimport argparse\nimport math\n\n# pytorch imports\nimport torch\nfrom torch.utils.data import DataLoader, sampler\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:42.654737Z","iopub.execute_input":"2022-12-30T10:46:42.655858Z","iopub.status.idle":"2022-12-30T10:46:42.666293Z","shell.execute_reply.started":"2022-12-30T10:46:42.655782Z","shell.execute_reply":"2022-12-30T10:46:42.664731Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"if not os.path.isdir(results_parent_dir):\n    os.mkdir(results_parent_dir)\nresults_dir = os.path.join(results_parent_dir, str(exp_code) + '_s{}'.format(seed))\nif not os.path.isdir(results_dir):\n    os.mkdir(results_dir)\n\nprint(results_dir)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:42.754315Z","iopub.execute_input":"2022-12-30T10:46:42.754761Z","iopub.status.idle":"2022-12-30T10:46:42.764365Z","shell.execute_reply.started":"2022-12-30T10:46:42.754724Z","shell.execute_reply":"2022-12-30T10:46:42.762399Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"/kaggle/working/results/exp_01_s1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"from modules.file_utils import save_pkl, load_pkl\n\nfrom modules.dataset_generic import Generic_WSI_Classification_Dataset, Generic_MIL_Dataset","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport pdb\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Sampler, WeightedRandomSampler, RandomSampler, SequentialSampler, sampler\nimport torch.optim as optim\nimport pdb\nimport torch.nn.functional as F\nimport math\nfrom itertools import islice\nimport collections\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass SubsetSequentialSampler(Sampler):\n    \"\"\"Samples elements sequentially from a given list of indices, without replacement.\n\n    Arguments:\n        indices (sequence): a sequence of indices\n    \"\"\"\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    def __len__(self):\n        return len(self.indices)\n\ndef collate_MIL(batch):\n    img = torch.cat([item[0] for item in batch], dim = 0)\n    label = torch.LongTensor([item[1] for item in batch])\n    return [img, label]\n\ndef collate_MIL_annot(batch):\n    img = torch.cat([item[0] for item in batch], dim = 0)\n    label = torch.LongTensor([item[1] for item in batch])\n    bool_annot = torch.LongTensor([item[2] for item in batch])\n    patch_annot = torch.LongTensor([item[3] for item in batch])\n    return [img, label, bool_annot, patch_annot]\n\ndef collate_features(batch):\n    img = torch.cat([item[0] for item in batch], dim = 0)\n    coords = np.vstack([item[1] for item in batch])\n    return [img, coords]\n\n\ndef get_simple_loader(dataset, batch_size=1, num_workers=1):\n    kwargs = {'num_workers': 4, 'pin_memory': False, 'num_workers': num_workers} if device.type == \"cuda\" else {}\n    loader = DataLoader(dataset, batch_size=batch_size, sampler = sampler.SequentialSampler(dataset), collate_fn = collate_MIL, **kwargs)\n    return loader\n\ndef get_split_loader(split_dataset, training = False, testing = False, weighted = False):\n    \"\"\"\n        return either the validation loader or training loader\n    \"\"\"\n    kwargs = {'num_workers': 4} if device.type == \"cuda\" else {}\n    if not testing:\n        if training:\n            if weighted:\n                weights = make_weights_for_balanced_classes_split(split_dataset)\n                loader = DataLoader(split_dataset, batch_size=1, sampler = WeightedRandomSampler(weights, len(weights)), collate_fn = collate_MIL, **kwargs)\n            else:\n\n                loader = DataLoader(split_dataset, batch_size=1, sampler = RandomSampler(split_dataset), collate_fn = collate_MIL_annot, **kwargs)\n        else:\n            loader = DataLoader(split_dataset, batch_size=1, sampler = SequentialSampler(split_dataset), collate_fn = collate_MIL, **kwargs)\n\n    else:\n        ids = np.random.choice(np.arange(len(split_dataset), int(len(split_dataset)*0.1)), replace = False)\n        loader = DataLoader(split_dataset, batch_size=1, sampler = SubsetSequentialSampler(ids), collate_fn = collate_MIL, **kwargs )\n\n    return loader\n\ndef get_optim(model, settings):\n    if settings['opt'] == \"adam\":\n        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=settings['lr'], weight_decay=settings['reg'])\n    elif settings['opt'] == 'sgd':\n        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=settings['lr'], momentum=0.9, weight_decay=setting['reg'])\n    else:\n        raise NotImplementedError\n    return optimizer\n\ndef print_network(net):\n    num_params = 0\n    num_params_train = 0\n    print(net)\n\n    for param in net.parameters():\n        n = param.numel()\n        num_params += n\n        if param.requires_grad:\n            num_params_train += n\n\n    print('Total number of parameters: %d' % num_params)\n    print('Total number of trainable parameters: %d' % num_params_train)\n\n\ndef generate_split(cls_ids, val_num, test_num, samples, n_splits = 5,\n    seed = 7, label_frac = 1.0, custom_test_ids = None):\n    indices = np.arange(samples).astype(int)\n\n    if custom_test_ids is not None:\n        indices = np.setdiff1d(indices, custom_test_ids)\n\n    np.random.seed(seed)\n    for i in range(n_splits):\n        all_val_ids = []\n        all_test_ids = []\n        sampled_train_ids = []\n\n        if custom_test_ids is not None: # pre-built test split, do not need to sample\n            all_test_ids.extend(custom_test_ids)\n\n        for c in range(len(val_num)):\n            possible_indices = np.intersect1d(cls_ids[c], indices) #all indices of this class\n            val_ids = np.random.choice(possible_indices, val_num[c], replace = False) # validation ids\n\n            remaining_ids = np.setdiff1d(possible_indices, val_ids) #indices of this class left after validation\n            all_val_ids.extend(val_ids)\n\n            if custom_test_ids is None: # sample test split\n\n                test_ids = np.random.choice(remaining_ids, test_num[c], replace = False)\n                remaining_ids = np.setdiff1d(remaining_ids, test_ids)\n                all_test_ids.extend(test_ids)\n\n            if label_frac == 1:\n                sampled_train_ids.extend(remaining_ids)\n\n            else:\n                sample_num  = math.ceil(len(remaining_ids) * label_frac)\n                slice_ids = np.arange(sample_num)\n                sampled_train_ids.extend(remaining_ids[slice_ids])\n\n        yield sampled_train_ids, all_val_ids, all_test_ids\n\n\ndef nth(iterator, n, default=None):\n    if n is None:\n        return collections.deque(iterator, maxlen=0)\n    else:\n        return next(islice(iterator,n, None), default)\n\ndef calculate_error(Y_hat, Y):\n    error = 1. - Y_hat.float().eq(Y.float()).float().mean().item()\n\n    return error\n\ndef make_weights_for_balanced_classes_split(dataset):\n    N = float(len(dataset))\n    weight_per_class = [N/len(dataset.slide_cls_ids[c]) for c in range(len(dataset.slide_cls_ids))]\n    weight = [0] * int(N)\n    for idx in range(len(dataset)):\n        y = dataset.getlabel(idx)\n        weight[idx] = weight_per_class[y]\n\n    return torch.DoubleTensor(weight)\n\ndef initialize_weights(module):\n    for m in module.modules():\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            m.bias.data.zero_()\n\n        elif isinstance(m, nn.BatchNorm1d):\n            nn.init.constant_(m.weight, 1)\n\n            nn.init.constant_(m.bias, 0)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:43.241238Z","iopub.execute_input":"2022-12-30T10:46:43.241678Z","iopub.status.idle":"2022-12-30T10:46:43.286171Z","shell.execute_reply.started":"2022-12-30T10:46:43.241645Z","shell.execute_reply":"2022-12-30T10:46:43.284392Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"#from modules.model_mil import MIL_fc, MIL_fc_mc\n\nfrom modules.utils import initialize_weights","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\"\"\"\nAttention Network without Gating (2 fc layers)\nargs:\n    L: input feature dimension\n    D: hidden layer dimension\n    dropout: whether to use dropout (p = 0.25)\n    n_classes: number of classes\n\"\"\"\nclass Attn_Net(nn.Module):\n\n    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):\n        super(Attn_Net, self).__init__()\n        self.module = [\n            nn.Linear(L, D),\n            nn.Tanh()]\n\n        if dropout:\n            self.module.append(nn.Dropout(0.25))\n\n        self.module.append(nn.Linear(D, n_classes))\n\n        self.module = nn.Sequential(*self.module)\n\n    def forward(self, x):\n        return self.module(x), x # N x n_classes\n\n\"\"\"\nAttention Network with Sigmoid Gating (3 fc layers)\nargs:\n    L: input feature dimension\n    D: hidden layer dimension\n    dropout: whether to use dropout (p = 0.25)\n    n_classes: number of classes\n\"\"\"\nclass Attn_Net_Gated(nn.Module):\n    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):\n        super(Attn_Net_Gated, self).__init__()\n        self.attention_a = [\n            nn.Linear(L, D),\n            nn.Tanh()]\n\n        self.attention_b = [nn.Linear(L, D),\n                            nn.Sigmoid()]\n        if dropout:\n            self.attention_a.append(nn.Dropout(0.25))\n            self.attention_b.append(nn.Dropout(0.25))\n\n        self.attention_a = nn.Sequential(*self.attention_a)\n        self.attention_b = nn.Sequential(*self.attention_b)\n\n        self.attention_c = nn.Linear(D, n_classes)\n\n    def forward(self, x):\n        a = self.attention_a(x)\n        b = self.attention_b(x)\n        A = a.mul(b)\n        A = self.attention_c(A)  # N x n_classes\n        return A, x\n\n\"\"\"\nargs:\n    gate: whether to use gated attention network\n    size_arg: config for network size\n    dropout: whether to use dropout\n    k_sample: number of positive/neg patches to sample for instance-level training\n    dropout: whether to use dropout (p = 0.25)\n    n_classes: number of classes\n    instance_loss_fn: loss function to supervise instance-level training\n    subtyping: whether it's a subtyping problem\n\"\"\"\nclass CLAM_SB(nn.Module):\n    def __init__(self, gate = True, size_arg = \"small\", dropout = False, k_sample=8, n_classes=2,\n        instance_loss_fn=nn.CrossEntropyLoss(), subtyping=False):\n        super(CLAM_SB, self).__init__()\n        self.size_dict = {\"small\": [1024, 512, 256], \"big\": [1024, 512, 384]}\n        size = self.size_dict[size_arg]\n        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]\n        if dropout:\n            fc.append(nn.Dropout(0.25))\n        if gate:\n            attention_net = Attn_Net_Gated(L = size[1], D = size[2], dropout = dropout, n_classes = 1)\n        else:\n            attention_net = Attn_Net(L = size[1], D = size[2], dropout = dropout, n_classes = 1)\n        fc.append(attention_net)\n        self.attention_net = nn.Sequential(*fc)\n        self.classifiers = nn.Linear(size[1], n_classes)\n        instance_classifiers = [nn.Linear(size[1], 2) for i in range(n_classes)]\n        self.instance_classifiers = nn.ModuleList(instance_classifiers)\n        self.k_sample = k_sample\n        self.instance_loss_fn = instance_loss_fn\n        self.n_classes = n_classes\n        self.subtyping = subtyping\n\n        initialize_weights(self)\n\n    def relocate(self):\n        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.attention_net = self.attention_net.to(device)\n        self.classifiers = self.classifiers.to(device)\n        self.instance_classifiers = self.instance_classifiers.to(device)\n\n    @staticmethod\n    def create_positive_targets(length, device):\n        return torch.full((length, ), 1, device=device).long()\n    @staticmethod\n    def create_negative_targets(length, device):\n        return torch.full((length, ), 0, device=device).long()\n\n    #instance-level evaluation for in-the-class attention branch\n    def inst_eval(self, A, h, classifier, bool_annot, patch_annot, alpha_weight, weight_alpha):\n        device=h.device\n        if len(A.shape) == 1:\n            A = A.view(1, -1)\n\n        # Get instance\n        top_p_ids = torch.topk(A.squeeze(), self.k_sample)[1]\n        top_n_ids = torch.topk(-A.squeeze(), self.k_sample)[1]\n        top_p = torch.index_select(h, dim=0, index=top_p_ids)\n        top_n = torch.index_select(h, dim=0, index=top_n_ids)\n        all_instances = torch.cat([top_p, top_n], dim=0)\n\n        logits = classifier(all_instances)\n        logits = logits.view(2*self.k_sample, 2)  # Shape is [24, 2]; B=12\n\n        all_preds = torch.topk(logits, 1, dim = 1)[1].squeeze(1)\n\n        # Get target labels\n        if bool_annot:\n            p_targets = torch.index_select(patch_annot.squeeze(), dim=0, index=top_p_ids).long()\n            n_targets = torch.index_select(patch_annot.squeeze(), dim=0, index=top_n_ids).long()\n        else:\n            p_targets = self.create_positive_targets(self.k_sample, device)\n            n_targets = self.create_negative_targets(self.k_sample, device)\n\n        all_targets = torch.cat([p_targets, n_targets], dim=0)\n#         print(\"logits\", logits.shape)\n#         print(\"all_targets\", all_targets.shape)\n        instance_loss = self.instance_loss_fn(logits, all_targets)\n        if alpha_weight and not bool_annot:\n            instance_loss *= weight_alpha\n        return instance_loss, all_preds, all_targets\n\n    #instance-level evaluation for out-of-the-class attention branch\n    def inst_eval_out(self, A, h, classifier):\n        device=h.device\n        if len(A.shape) == 1:\n            A = A.view(1, -1)\n        top_p_ids = torch.topk(A.squeeze(), self.k_sample)[1]\n        top_p = torch.index_select(h, dim=0, index=top_p_ids)\n        p_targets = self.create_negative_targets(self.k_sample, device)\n#         print(\"top_p\", top_p.shape)\n        logits = classifier(top_p)\n        p_preds = torch.topk(logits, 1, dim = 1)[1].squeeze(1)\n#         print(\"logits\", logits.shape)\n#         print(\"p_targets\", p_targets.shape)\n        instance_loss = self.instance_loss_fn(logits.squeeze(), p_targets)\n        return instance_loss, p_preds, p_targets\n\n    def forward(self, h, bool_annot=None, patch_annot=None, alpha_weight=False, weight_alpha=None, label=None, instance_eval=False, return_features=False, attention_only=False):\n        device = h.device\n#         print(\"h.shape\", h.shape)\n        A, h = self.attention_net(h)  # NxK\n#         print(\"A.shape\", A.shape)\n#         print(\"h.shape\", h.shape)\n        A = torch.transpose(A, 1, 0)  # KxN\n        if attention_only:\n            return A\n        A_raw = A\n        A = F.softmax(A, dim=1)  # softmax over N\n\n        if instance_eval:\n            total_inst_loss = 0.0\n            all_preds = []\n            all_targets = []\n            inst_labels = F.one_hot(label, num_classes=self.n_classes).squeeze() #binarize label\n            for i in range(len(self.instance_classifiers)):\n                inst_label = inst_labels[i].item()\n#                 print(\"inst_label\", inst_label)\n                classifier = self.instance_classifiers[i]\n                if inst_label == 1: #in-the-class:\n                    instance_loss, preds, targets = self.inst_eval(A, h, classifier, bool_annot, patch_annot, alpha_weight, weight_alpha)\n#                     print(\"1, preds\", preds.shape)\n#                     print(\"1, targets\", targets.shape)\n                    all_preds.extend(preds.cpu().numpy())\n                    all_targets.extend(targets.cpu().numpy())\n                else: #out-of-the-class\n                    if self.subtyping:\n                        instance_loss, preds, targets = self.inst_eval_out(A, h, classifier)\n#                         print(\"0, preds\", preds.shape)\n#                         print(\"0, targets\", targets.shape)\n#                         print(\"0, all_preds\", all_preds)\n#                         print(\"0, all_targets\", all_targets)\n                        all_preds.extend(preds.squeeze().cpu().numpy())\n                        all_targets.extend(targets.cpu().numpy())\n#                         print(\"0, all_preds\", all_preds)\n#                         print(\"0, all_targets\", all_targets)\n                    else:\n                        continue\n                total_inst_loss += instance_loss\n\n            if self.subtyping:\n                total_inst_loss /= len(self.instance_classifiers)\n\n        M = torch.mm(A.view(1, 24), h.view(24, 512))\n        logits = self.classifiers(M)\n        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n#         print(\"logits\", logits)\n#         print(\"logits.shape\", logits.shape)\n#         print(\"Y_hat\", Y_hat)\n#         print(\"Y_hat.shape\", Y_hat.shape)\n\n#         print(\"all_targets\", all_targets)\n#         print(\"all_targets shape\", len(all_targets))\n#         print(\"all_preds\", all_preds)\n#         print(\"all_preds shape\", len(all_preds))\n        Y_prob = F.softmax(logits, dim = 1)\n#         print(\"Y_prob.shape\", Y_prob.shape)\n#         print(\"Y_prob\", logits)\n        if instance_eval:\n            results_dict = {'instance_loss': total_inst_loss, 'inst_labels': np.array(all_targets),\n            'inst_preds': np.array(all_preds)}\n        else:\n            results_dict = {}\n        if return_features:\n            results_dict.update({'features': M})\n#         print(\"Y_hat shape\", Y_hat.shape)\n        return logits, Y_prob, Y_hat, A_raw, results_dict\n\nclass CLAM_MB(CLAM_SB):\n    def __init__(self, gate = True, size_arg = \"small\", dropout = False, k_sample=8, n_classes=2,\n        instance_loss_fn=nn.CrossEntropyLoss(), subtyping=False):\n        nn.Module.__init__(self)\n        self.size_dict = {\"small\": [1024, 512, 256], \"big\": [1024, 512, 384]}\n        size = self.size_dict[size_arg]\n        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]\n        if dropout:\n            fc.append(nn.Dropout(0.25))\n        if gate:\n            attention_net = Attn_Net_Gated(L = size[1], D = size[2], dropout = dropout, n_classes = n_classes)\n        else:\n            attention_net = Attn_Net(L = size[1], D = size[2], dropout = dropout, n_classes = n_classes)\n        fc.append(attention_net)\n        self.attention_net = nn.Sequential(*fc)\n        bag_classifiers = [nn.Linear(size[1], 1) for i in range(n_classes)] #use an indepdent linear layer to predict each class\n        self.classifiers = nn.ModuleList(bag_classifiers)\n        instance_classifiers = [nn.Linear(size[1], 2) for i in range(n_classes)]\n        self.instance_classifiers = nn.ModuleList(instance_classifiers)\n        self.k_sample = k_sample\n        self.instance_loss_fn = instance_loss_fn\n        self.n_classes = n_classes\n        self.subtyping = subtyping\n        initialize_weights(self)\n\n    def forward(self, h, label=None, instance_eval=False, return_features=False, attention_only=False):\n        device = h.device\n        A, h = self.attention_net(h)  # NxK\n        A = torch.transpose(A, 1, 0)  # KxN\n        if attention_only:\n            return A\n        A_raw = A\n        A = F.softmax(A, dim=1)  # softmax over N\n\n        if instance_eval:\n            total_inst_loss = 0.0\n            all_preds = []\n            all_targets = []\n            inst_labels = F.one_hot(label, num_classes=self.n_classes).squeeze() #binarize label\n            for i in range(len(self.instance_classifiers)):\n                inst_label = inst_labels[i].item()\n                classifier = self.instance_classifiers[i]\n                if inst_label == 1: #in-the-class:\n                    instance_loss, preds, targets = self.inst_eval(A.view(self.n_classes, 24)[i], h, classifier)\n                    all_preds.extend(preds.cpu().numpy())\n                    all_targets.extend(targets.cpu().numpy())\n                else: #out-of-the-class\n                    if self.subtyping:\n                        instance_loss, preds, targets = self.inst_eval_out(A.view(self.n_classes, 24)[i], h, classifier)\n                        all_preds.extend(preds.cpu().numpy())\n                        all_targets.extend(targets.cpu().numpy())\n                    else:\n                        continue\n                total_inst_loss += instance_loss\n\n            if self.subtyping:\n                total_inst_loss /= len(self.instance_classifiers)\n\n        M = torch.mm(A.view(self.n_classes, 24), h.view(24, 512))\n        logits = torch.empty(1, self.n_classes).float().to(device)\n        for c in range(self.n_classes):\n            logits[0, c] = self.classifiers[c](M[c])\n        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n        Y_prob = F.softmax(logits, dim = 1)\n        if instance_eval:\n            results_dict = {'instance_loss': total_inst_loss, 'inst_labels': np.array(all_targets),\n            'inst_preds': np.array(all_preds)}\n        else:\n            results_dict = {}\n        if return_features:\n            results_dict.update({'features': M})\n        return logits, Y_prob, Y_hat, A_raw, results_dict","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:43.854803Z","iopub.execute_input":"2022-12-30T10:46:43.855371Z","iopub.status.idle":"2022-12-30T10:46:43.925425Z","shell.execute_reply.started":"2022-12-30T10:46:43.855330Z","shell.execute_reply":"2022-12-30T10:46:43.923460Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport os\n\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import auc as calc_auc\n\nclass Accuracy_Logger(object):\n    \"\"\"Accuracy logger\"\"\"\n    def __init__(self, n_classes):\n        super(Accuracy_Logger, self).__init__()\n        self.n_classes = n_classes\n        self.initialize()\n\n    def initialize(self):\n        self.data = [{\"count\": 0, \"correct\": 0} for i in range(self.n_classes)]\n\n    def log(self, Y_hat, Y):\n        Y_hat = int(Y_hat)\n        Y = int(Y)\n        self.data[Y][\"count\"] += 1\n        self.data[Y][\"correct\"] += (Y_hat == Y)\n\n    def log_batch(self, Y_hat, Y):\n#         print(\"Y_hat\", Y_hat)\n        Y_hat = np.array(Y_hat).astype(int)\n        Y = np.array(Y).astype(int)\n        for label_class in np.unique(Y):\n            cls_mask = Y == label_class\n#             print(\"B-Log Y_hat\", Y_hat)\n#             print(\"B-Log Y_hat.shape\", Y_hat.shape)\n#             print(\"B-Log Y.shape\", Y.shape)\n#             print(\"B-0\", sum(cls_mask))\n#             print(\"B-1\", sum(Y_hat[cls_mask] == Y[cls_mask]))\n            self.data[label_class][\"count\"] += sum(cls_mask)\n            self.data[label_class][\"correct\"] += sum(Y_hat[cls_mask] == Y[cls_mask])\n\n#         Y_hat = np.array(Y_hat).astype(int)\n#         Y_hat = np.reshape(Y_hat, (16, 2))\n#         Y_hat = Y_hat[:, 0]\n#         Y = np.array(Y).astype(int)\n#         for label_class in np.unique(Y):\n#             cls_mask = [Y == label_class]\n#             self.data[label_class][\"count\"] += sum(cls_mask)\n#             self.data[label_class][\"correct\"] += sum(tuple([(Y_hat[cls_mask] == Y[cls_mask]) ]) )\n\n#             print(\"B count:\", self.data[label_class][\"count\"])\n#             print(\"B correct:\", self.data[label_class][\"correct\"])\n\n    def get_summary(self, c):\n        count = self.data[c][\"count\"]\n        correct = self.data[c][\"correct\"]\n\n        if count == 0:\n            acc = None\n        else:\n            acc = float(correct) / count\n\n        return acc, correct, count\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=20, stop_epoch=50, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 20\n            stop_epoch (int): Earliest epoch possible for stopping\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.stop_epoch = stop_epoch\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, epoch, val_loss, model, ckpt_name = 'checkpoint.pt'):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, ckpt_name)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience and epoch > self.stop_epoch:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, ckpt_name)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, ckpt_name):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), ckpt_name)\n        self.val_loss_min = val_loss\n\ndef train(datasets, cur, settings):\n    \"\"\"\n        train for a single fold\n    \"\"\"\n    print(\"Settings:\", settings)\n    print('\\nTraining Fold {}!'.format(cur))\n    exp_dir = os.path.join(settings[\"results_dir\"], str(settings[\"exp_code\"]) + '_s{}'.format(settings[\"seed\"]))\n    if not os.path.isdir(exp_dir):\n        os.mkdir(exp_dir)\n    split_dir = os.path.join(exp_dir, 'splits_{}'.format(cur))\n    if not os.path.isdir(split_dir):\n        os.mkdir(split_dir)\n\n    if settings['log_data']:\n        writer_dir = os.path.join(exp_dir, \"logs\", str(cur))\n        if not os.path.isdir(writer_dir):\n            os.mkdir(writer_dir)\n\n        from tensorboardX import SummaryWriter\n        writer = SummaryWriter(writer_dir, flush_secs=15)\n\n    else:\n        writer = None\n\n    print('\\nInit train/val/test splits...', end=' ')\n    train_split, val_split, test_split = datasets\n    save_splits(datasets, ['train', 'val', 'test'], os.path.join(split_dir, 'splits_{}.csv'.format(cur)), annot_create=False)\n    print('Done!')\n    print(\"Training on {} samples\".format(len(train_split)))\n    print(\"Validating on {} samples\".format(len(val_split)))\n    print(\"Testing on {} samples\".format(len(test_split)))\n\n    print('\\nInit loss function...', end=' ')\n    if settings['bag_loss'] == 'svm':\n        from topk.svm import SmoothTop1SVM\n        loss_fn = SmoothTop1SVM(n_classes = settings['n_classes'])\n        if device.type == 'cuda':\n            loss_fn = loss_fn.cuda()\n    else:\n        loss_fn = nn.CrossEntropyLoss()\n    print('Done!')\n\n    print('\\nInit Model...', end=' ')\n    model_dict = {\"dropout\": settings['dropout'], 'n_classes': settings['n_classes']}\n    if settings['model_type'] == 'clam' and settings['subtyping']:\n        model_dict.update({'subtyping': True})\n\n    if settings['model_size'] is not None and settings['model_type'] != 'mil':\n        model_dict.update({\"size_arg\": settings['model_size']})\n\n    if settings['model_type'] in ['clam_sb', 'clam_mb']:\n        if settings['subtyping']:\n            model_dict.update({'subtyping': True})\n\n        if settings['B'] > 0:\n            model_dict.update({'k_sample': settings['B']})\n\n        if settings['inst_loss'] == 'svm':\n            from topk.svm import SmoothTop1SVM\n            instance_loss_fn = SmoothTop1SVM(n_classes = 2)\n            if device.type == 'cuda':\n                instance_loss_fn = instance_loss_fn.cuda()\n        else:\n            instance_loss_fn = nn.CrossEntropyLoss()\n\n        if settings['model_type'] =='clam_sb':\n            model = CLAM_SB(**model_dict, instance_loss_fn=instance_loss_fn)\n        elif settings['model_type'] == 'clam_mb':\n            model = CLAM_MB(**model_dict, instance_loss_fn=instance_loss_fn)\n        else:\n            raise NotImplementedError\n\n    else: # settings['model_type == 'mil'\n        if settings['n_classes'] > 2:\n            model = MIL_fc_mc(**model_dict)\n        else:\n            model = MIL_fc(**model_dict)\n\n    model.relocate()\n    print('Done!')\n    print_network(model)\n\n    print('\\nInit optimizer ...', end=' ')\n    optimizer = get_optim(model, settings)\n    print('Done!')\n\n    print('\\nInit Loaders...', end=' ')\n    train_loader = get_split_loader(train_split, training=True, testing = settings['testing'], weighted = settings['weighted_sample'])\n    val_loader = get_split_loader(val_split,  testing = settings['testing'])\n    test_loader = get_split_loader(test_split, testing = settings['testing'])\n    print('Done!')\n\n    print('\\nSetup EarlyStopping...', end=' ')\n    if settings['early_stopping']:\n        early_stopping = EarlyStopping(patience = 20, stop_epoch=50, verbose = True)\n\n    else:\n        early_stopping = None\n    print('Done!')\n    \n    for epoch in range(settings['max_epochs']):\n        weight_alpha = get_alpha_weight(epoch, settings['T1'], settings['T2'], settings['af'])\n        if settings['model_type'] in ['clam_sb', 'clam_mb'] and not settings['no_inst_cluster']:\n            train_loop_clam(epoch, model, train_loader, optimizer, settings['n_classes'], settings['bag_weight'], writer, loss_fn, alpha_weight=settings['alpha_weight'], weight_alpha=weight_alpha)\n            stop = validate_clam(cur, epoch, model, val_loader, settings['n_classes'],\n                early_stopping, writer, loss_fn, settings['results_dir'])\n\n        else:\n            train_loop(epoch, model, train_loader, optimizer, settings['n_classes'], writer, loss_fn)\n            stop = validate(cur, epoch, model, val_loader, settings['n_classes'],\n                early_stopping, writer, loss_fn, settings['results_dir'])\n\n        if stop:\n            break\n\n    exp_dir = os.path.join(settings[\"results_dir\"], str(settings[\"exp_code\"]) + '_s{}'.format(settings[\"seed\"]))\n    split_dir = os.path.join(exp_dir, 'splits_{}'.format(cur))\n    if settings['early_stopping']:\n        model.load_state_dict(torch.load(os.path.join(split_dir, \"s_{}_checkpoint.pt\".format(cur))))\n    else:\n        torch.save(model.state_dict(), os.path.join(split_dir, \"s_{}_checkpoint.pt\".format(cur)))\n\n    _, val_error, val_auc, _= summary(model, val_loader, settings['n_classes'])\n    print('Val error: {:.4f}, ROC AUC: {:.4f}'.format(val_error, val_auc))\n\n    results_dict, test_error, test_auc, acc_logger = summary(model, test_loader, settings['n_classes'])\n    print('Test error: {:.4f}, ROC AUC: {:.4f}'.format(test_error, test_auc))\n\n    for i in range(settings['n_classes']):\n        acc, correct, count = acc_logger.get_summary(i)\n        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n\n        if writer:\n            writer.add_scalar('final/test_class_{}_acc'.format(i), acc, 0)\n\n    if writer:\n        writer.add_scalar('final/val_error', val_error, 0)\n        writer.add_scalar('final/val_auc', val_auc, 0)\n        writer.add_scalar('final/test_error', test_error, 0)\n        writer.add_scalar('final/test_auc', test_auc, 0)\n        writer.close()\n    return results_dict, test_auc, val_auc, 1-test_error, 1-val_error\n\n\ndef train_loop_clam(epoch, model, loader, optimizer, n_classes, bag_weight, writer = None, loss_fn = None, alpha_weight=False, weight_alpha=None):\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.train()\n    acc_logger = Accuracy_Logger(n_classes=n_classes)\n    inst_logger = Accuracy_Logger(n_classes=n_classes)\n\n    train_loss = 0.\n    train_error = 0.\n    train_inst_loss = 0.\n    inst_count = 0\n\n    print('\\n')\n    for batch_idx, (data, label, bool_annot, patch_annot) in enumerate(loader):\n        data = data.float()\n        model = model.float()\n        data, label = data.to(device), label.to(device)\n        bool_annot, patch_annot = bool_annot.to(device), patch_annot.to(device)\n        # print(\"data.shape\", data.shape)\n        logits, Y_prob, Y_hat, _, instance_dict = model(data, bool_annot=bool_annot, patch_annot=patch_annot, label=label, alpha_weight=alpha_weight, weight_alpha=weight_alpha, instance_eval=True)\n\n        acc_logger.log(Y_hat, label)\n        loss = loss_fn(logits.view(1, 2), label)\n        loss_value = loss.item()\n\n        instance_loss = instance_dict['instance_loss']\n        inst_count+=1\n        instance_loss_value = instance_loss.item()\n        train_inst_loss += instance_loss_value\n\n        total_loss = bag_weight * loss + (1-bag_weight) * instance_loss\n\n        inst_preds = instance_dict['inst_preds']\n        inst_labels = instance_dict['inst_labels']\n#         print(\"inst_preds\", inst_preds.shape)\n#         print(\"inst_labels\", inst_labels.shape)\n        inst_logger.log_batch(inst_preds, inst_labels)\n\n        train_loss += loss_value\n        if (batch_idx + 1) % 20 == 0:\n            print('batch {}, loss: {:.4f}, instance_loss: {:.4f}, weighted_loss: {:.4f}, '.format(batch_idx, loss_value, instance_loss_value, total_loss.item()) +\n                'label: {}, bag_size: {}'.format(label.item(), data.size(0)))\n\n        error = calculate_error(Y_hat, label)\n        train_error += error\n\n        # backward pass\n        total_loss.backward()\n        # step\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # calculate loss and error for epoch\n    train_loss /= len(loader)\n    train_error /= len(loader)\n\n    if inst_count > 0:\n        train_inst_loss /= inst_count\n        print('\\n')\n        for i in range(2):\n            acc, correct, count = inst_logger.get_summary(i)\n            print('class {} clustering acc {}: correct {}/{}'.format(i, acc, correct, count))\n\n    print('Epoch: {}, train_loss: {:.4f}, train_clustering_loss:  {:.4f}, train_error: {:.4f}'.format(epoch, train_loss, train_inst_loss,  train_error))\n    for i in range(n_classes):\n        acc, correct, count = acc_logger.get_summary(i)\n        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n        if writer and acc is not None:\n            writer.add_scalar('train/class_{}_acc'.format(i), acc, epoch)\n\n    if writer:\n        writer.add_scalar('train/loss', train_loss, epoch)\n        writer.add_scalar('train/error', train_error, epoch)\n        writer.add_scalar('train/clustering_loss', train_inst_loss, epoch)\n\ndef train_loop(epoch, model, loader, optimizer, n_classes, writer = None, loss_fn = None):\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.train()\n    acc_logger = Accuracy_Logger(n_classes=n_classes)\n    train_loss = 0.\n    train_error = 0.\n\n    print('\\n')\n    for batch_idx, (data, label) in enumerate(loader):\n        data, label = data.to(device), label.to(device)\n\n        logits, Y_prob, Y_hat, _, _ = model(data)\n\n        acc_logger.log(Y_hat, label)\n        loss = loss_fn(logits.view(1, 2), label)\n        loss_value = loss.item()\n\n        train_loss += loss_value\n        if (batch_idx + 1) % 20 == 0:\n            print('batch {}, loss: {:.4f}, label: {}, bag_size: {}'.format(batch_idx, loss_value, label.item(), data.size(0)))\n\n        error = calculate_error(Y_hat, label)\n        train_error += error\n\n        # backward pass\n        loss.backward()\n        # step\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # calculate loss and error for epoch\n    train_loss /= len(loader)\n    train_error /= len(loader)\n\n    print('Epoch: {}, train_loss: {:.4f}, train_error: {:.4f}'.format(epoch, train_loss, train_error))\n    for i in range(n_classes):\n        acc, correct, count = acc_logger.get_summary(i)\n        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n        if writer:\n            writer.add_scalar('train/class_{}_acc'.format(i), acc, epoch)\n\n    if writer:\n        writer.add_scalar('train/loss', train_loss, epoch)\n        writer.add_scalar('train/error', train_error, epoch)\n\n\ndef validate(cur, epoch, model, loader, n_classes, early_stopping = None, writer = None, loss_fn = None, results_dir=None):\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    acc_logger = Accuracy_Logger(n_classes=n_classes)\n    # loader.dataset.update_mode(True)\n    val_loss = 0.\n    val_error = 0.\n\n    prob = np.zeros((len(loader), n_classes))\n    labels = np.zeros(len(loader))\n\n    with torch.no_grad():\n        for batch_idx, (data, label) in enumerate(loader):\n            data, label = data.to(device, non_blocking=True), label.to(device, non_blocking=True)\n\n            logits, Y_prob, Y_hat, _, _ = model(data)\n\n            acc_logger.log(Y_hat, label)\n\n            loss = loss_fn(logits.view(1, 2), label)\n\n            prob[batch_idx] = Y_prob.cpu().numpy()\n            labels[batch_idx] = label.item()\n\n            val_loss += loss.item()\n            error = calculate_error(Y_hat, label)\n            val_error += error\n\n\n    val_error /= len(loader)\n    val_loss /= len(loader)\n\n    if n_classes == 2:\n        auc = roc_auc_score(labels, prob[:, 1])\n\n    else:\n        auc = roc_auc_score(labels, prob, multi_class='ovr')\n\n\n    if writer:\n        writer.add_scalar('val/loss', val_loss, epoch)\n        writer.add_scalar('val/auc', auc, epoch)\n        writer.add_scalar('val/error', val_error, epoch)\n\n    print('\\nVal Set, val_loss: {:.4f}, val_error: {:.4f}, auc: {:.4f}'.format(val_loss, val_error, auc))\n    for i in range(n_classes):\n        acc, correct, count = acc_logger.get_summary(i)\n        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n\n    if early_stopping:\n        exp_dir = os.path.join(settings[\"results_dir\"], str(settings[\"exp_code\"]) + '_s{}'.format(settings[\"seed\"]))\n        assert exp_dir\n        split_dir = os.path.join(exp_dir, 'splits_{}'.format(cur))\n        assert split_dir\n        early_stopping(epoch, val_loss, model, ckpt_name = os.path.join(split_dir, \"s_{}_checkpoint.pt\".format(cur)))\n\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            return True\n\n    return False\n\ndef validate_clam(cur, epoch, model, loader, n_classes, early_stopping = None, writer = None, loss_fn = None, results_dir = None):\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    acc_logger = Accuracy_Logger(n_classes=n_classes)\n    inst_logger = Accuracy_Logger(n_classes=n_classes)\n    val_loss = 0.\n    val_error = 0.\n\n    val_inst_loss = 0.\n    val_inst_acc = 0.\n    inst_count=0\n\n    prob = np.zeros((len(loader), n_classes))\n    labels = np.zeros(len(loader))\n    sample_size = model.k_sample\n    with torch.no_grad():\n        for batch_idx, (data, label) in enumerate(loader):\n            data, label = data.to(device), label.to(device)\n            logits, Y_prob, Y_hat, _, instance_dict = model(data, label=label, instance_eval=True)\n            acc_logger.log(Y_hat, label)\n\n            loss = loss_fn(logits.view(1, 2), label)\n\n            val_loss += loss.item()\n\n            instance_loss = instance_dict['instance_loss']\n\n            inst_count+=1\n            instance_loss_value = instance_loss.item()\n            val_inst_loss += instance_loss_value\n\n            inst_preds = instance_dict['inst_preds']\n            inst_labels = instance_dict['inst_labels']\n            inst_logger.log_batch(inst_preds, inst_labels)\n\n            prob[batch_idx] = Y_prob.cpu().numpy()\n            labels[batch_idx] = label.item()\n\n            error = calculate_error(Y_hat, label)\n            val_error += error\n\n    val_error /= len(loader)\n    val_loss /= len(loader)\n\n    if n_classes == 2:\n        auc = roc_auc_score(labels, prob[:, 1])\n        aucs = []\n    else:\n        aucs = []\n        binary_labels = label_binarize(labels, classes=[i for i in range(n_classes)])\n        for class_idx in range(n_classes):\n            if class_idx in labels:\n                fpr, tpr, _ = roc_curve(binary_labels[:, class_idx], prob[:, class_idx])\n                aucs.append(calc_auc(fpr, tpr))\n            else:\n                aucs.append(float('nan'))\n\n        auc = np.nanmean(np.array(aucs))\n\n    print('\\nVal Set, val_loss: {:.4f}, val_error: {:.4f}, auc: {:.4f}'.format(val_loss, val_error, auc))\n    if inst_count > 0:\n        val_inst_loss /= inst_count\n        for i in range(2):\n            acc, correct, count = inst_logger.get_summary(i)\n            print('class {} clustering acc {}: correct {}/{}'.format(i, acc, correct, count))\n\n    if writer:\n        writer.add_scalar('val/loss', val_loss, epoch)\n        writer.add_scalar('val/auc', auc, epoch)\n        writer.add_scalar('val/error', val_error, epoch)\n        writer.add_scalar('val/inst_loss', val_inst_loss, epoch)\n\n\n    for i in range(n_classes):\n        acc, correct, count = acc_logger.get_summary(i)\n        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n\n        if writer and acc is not None:\n            writer.add_scalar('val/class_{}_acc'.format(i), acc, epoch)\n\n\n    if early_stopping:\n        exp_dir = os.path.join(settings[\"results_dir\"], str(settings[\"exp_code\"]) + '_s{}'.format(settings[\"seed\"]))\n        assert exp_dir\n        split_dir = os.path.join(exp_dir, 'splits_{}'.format(cur))\n        assert split_dir\n        early_stopping(epoch, val_loss, model, ckpt_name = os.path.join(split_dir, \"s_{}_checkpoint.pt\".format(cur)))\n\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            return True\n\n    return False\n\ndef summary(model, loader, n_classes):\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    acc_logger = Accuracy_Logger(n_classes=n_classes)\n    model.eval()\n    test_loss = 0.\n    test_error = 0.\n\n    all_probs = np.zeros((len(loader), n_classes))\n    all_labels = np.zeros(len(loader))\n\n    slide_ids = loader.dataset.slide_data['slide_id']\n    patient_results = {}\n\n    for batch_idx, (data, label) in enumerate(loader):\n        data, label = data.to(device), label.to(device)\n        slide_id = slide_ids.iloc[batch_idx]\n        with torch.no_grad():\n            logits, Y_prob, Y_hat, _, _ = model(data)\n\n        acc_logger.log(Y_hat, label)\n        probs = Y_prob.cpu().numpy()\n        all_probs[batch_idx] = probs\n        all_labels[batch_idx] = label.item()\n\n        patient_results.update({slide_id: {'slide_id': np.array(slide_id), 'prob': probs, 'label': label.item()}})\n        error = calculate_error(Y_hat, label)\n        test_error += error\n\n    test_error /= len(loader)\n\n    if n_classes == 2:\n        auc = roc_auc_score(all_labels, all_probs[:, 1])\n        aucs = []\n    else:\n        aucs = []\n        binary_labels = label_binarize(all_labels, classes=[i for i in range(n_classes)])\n        for class_idx in range(n_classes):\n            if class_idx in all_labels:\n                fpr, tpr, _ = roc_curve(binary_labels[:, class_idx], all_probs[:, class_idx])\n                aucs.append(calc_auc(fpr, tpr))\n            else:\n                aucs.append(float('nan'))\n\n        auc = np.nanmean(np.array(aucs))\n\n\n    return patient_results, test_error, auc, acc_logger\n\ndef get_alpha_weight(epoch, T1, T2, af):\n    if epoch < T1:\n        return 0.0\n    elif epoch > T2:\n        return af\n    else:\n         return ((epoch-T1) / (T2-T1))*af","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:44.009436Z","iopub.execute_input":"2022-12-30T10:46:44.009896Z","iopub.status.idle":"2022-12-30T10:46:44.135025Z","shell.execute_reply.started":"2022-12-30T10:46:44.009859Z","shell.execute_reply":"2022-12-30T10:46:44.133272Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=7):\n    import random\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if device.type == 'cuda':\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:44.138425Z","iopub.execute_input":"2022-12-30T10:46:44.139072Z","iopub.status.idle":"2022-12-30T10:46:44.149479Z","shell.execute_reply.started":"2022-12-30T10:46:44.139028Z","shell.execute_reply":"2022-12-30T10:46:44.147748Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nseed_torch(seed)\n\nencoding_size = 1024\nsettings = {\n    'k': k,\n    'k_start': k_start,\n    'k_end': k_end,\n    'task': task,\n    'max_epochs': max_epochs,\n    'results_dir': results_dir,\n    'lr': lr,\n    'experiment': exp_code,\n    'reg': reg,\n    'label_frac': label_frac,\n    'bag_loss': bag_loss,\n    'seed': seed,\n    'model_type': model_type,\n    'model_size': model_size,\n    \"use_drop_out\": drop_out,\n    'weighted_sample': weighted_sample,\n    'opt': opt,\n    'patch_dir': patch_dir,\n    'feat_dir': feat_dir,\n    'label_frac': label_frac,\n    'split_dir': split_dir,\n    'log_data': log_data,\n    'dataset_csv': dataset_csv,\n    'testing': testing,\n    'early_stopping': early_stopping,\n    'dropout': dropout,\n    'no_inst_cluster': no_inst_cluster,\n    'subtyping': subtyping,\n    'exp_code': exp_code,\n    'bag_weight': bag_weight,\n    'inst_loss': inst_loss,\n    'B': B,\n    'annot_dir': annot_dir,\n    'patch_annot_dir': patch_annot_dir,\n    'alpha_weight': alpha_weight,\n    'T1': T1,\n    'T2': T2,\n    'af': af\n}","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:44.336428Z","iopub.execute_input":"2022-12-30T10:46:44.336861Z","iopub.status.idle":"2022-12-30T10:46:44.351551Z","shell.execute_reply.started":"2022-12-30T10:46:44.336829Z","shell.execute_reply":"2022-12-30T10:46:44.350030Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(results_dir, 'config.yaml'), 'w') as yaml_file:\n    yaml.dump(settings, yaml_file, default_flow_style=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:44.546255Z","iopub.execute_input":"2022-12-30T10:46:44.546680Z","iopub.status.idle":"2022-12-30T10:46:44.557816Z","shell.execute_reply.started":"2022-12-30T10:46:44.546647Z","shell.execute_reply":"2022-12-30T10:46:44.556129Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"if task == 'task_fungal_vs_nonfungal':\n    n_classes = 2\n    settings.update({'n_classes': n_classes})\n    dataset = Generic_MIL_Dataset(csv_path=dataset_csv,\n                                  data_dir=feat_dir,\n                                  annot_dir=annot_dir,\n                                  patch_annot_dir=patch_annot_dir,\n                                  results_dir=results_dir,\n                                  shuffle=False,\n                                  seed=seed,\n                                  print_info=True,\n                                  label_dict={'nonfungal': 0, 'fungal': 1},\n                                  patient_strat=False,\n                                  ignore=[])\n\nelif task == 'task_1_tumor_vs_normal':\n    n_classes = 2\n    settings.update({'n_classes': n_classes})\n    dataset = Generic_MIL_Dataset(csv_path='dataset_csv/tumor_vs_normal_dummy_clean.csv',\n                                  data_dir=os.path.join(\n                                      data_root_dir, 'tumor_vs_normal_resnet_features'),\n                                  shuffle=False,\n                                  seed=seed,\n                                  print_info=True,\n                                  label_dict={'normal_tissue': 0,\n                                              'tumor_tissue': 1},\n                                  patient_strat=False,\n                                  ignore=[])\n\nelif task == 'task_2_tumor_subtyping':\n    n_classes = 3\n    settings.update({'n_classes': n_classes})\n    dataset = Generic_MIL_Dataset(csv_path='dataset_csv/tumor_subtyping_dummy_clean.csv',\n                                  data_dir=os.path.join(\n                                      data_root_dir, 'tumor_subtyping_resnet_features'),\n                                  shuffle=False,\n                                  seed=seed,\n                                  print_info=True,\n                                  label_dict={'subtype_1': 0,\n                                              'subtype_2': 1, 'subtype_3': 2},\n                                  patient_strat=False,\n                                  ignore=[])\n\n    if model_type in ['clam_sb', 'clam_mb']:\n        assert subtyping\n\nelse:\n    raise NotImplementedError\n\nwith open(results_dir + '/experiment_{}.txt'.format(exp_code), 'w') as f:\n    print(settings, file=f)\nf.close()\n\nprint(\"################# Settings ###################\")\nfor key, val in settings.items():\n    print(\"{}:  {}\".format(key, val))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-30T10:46:44.927563Z","iopub.execute_input":"2022-12-30T10:46:44.928063Z","iopub.status.idle":"2022-12-30T10:46:45.381450Z","shell.execute_reply.started":"2022-12-30T10:46:44.928024Z","shell.execute_reply":"2022-12-30T10:46:45.379103Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"      case_id  slide_id label\n0      case_0   F005a02     1\n1      case_1   F006a01     1\n2      case_2   F006a02     1\n3      case_3   F006a03     1\n4      case_4   F006a04     1\n..        ...       ...   ...\n418  case_418  N012a017     0\n419  case_419  N012a018     0\n420  case_420  N012a019     0\n421  case_421  N012a020     0\n422  case_422  N017a001     0\n\n[423 rows x 3 columns]\nlabel column: label\nlabel dictionary: {'nonfungal': 0, 'fungal': 1}\nnumber of classes: 2\nslide-level counts:  \n 1    208\n0    215\nName: label, dtype: int64\nPatient-LVL; Number of samples registered in class 0: 215\nSlide-LVL; Number of samples registered in class 0: 215\nPatient-LVL; Number of samples registered in class 1: 208\nSlide-LVL; Number of samples registered in class 1: 208\n################# Settings ###################\nk:  5\nk_start:  -1\nk_end:  -1\ntask:  task_fungal_vs_nonfungal\nmax_epochs:  200\nresults_dir:  /kaggle/working/results/exp_01_s1\nlr:  0.0001\nexperiment:  exp_01\nreg:  1e-05\nlabel_frac:  1.0\nbag_loss:  ce\nseed:  1\nmodel_type:  clam_sb\nmodel_size:  small\nuse_drop_out:  False\nweighted_sample:  False\nopt:  adam\npatch_dir:  /kaggle/working/patches\nfeat_dir:  /kaggle/working/features\nsplit_dir:  /kaggle/working/splits/fungal_vs_nonfungal_100/\nlog_data:  False\ndataset_csv:  /kaggle/working/fungal_vs_nonfungal.csv\ntesting:  False\nearly_stopping:  False\ndropout:  True\nno_inst_cluster:  False\nsubtyping:  False\nexp_code:  exp_01\nbag_weight:  0.5\ninst_loss:  None\nB:  12\nannot_dir:  /kaggle/input/fungal-10x-annot\npatch_annot_dir:  /kaggle/working/patch_annot\nalpha_weight:  False\nT1:  50\nT2:  150\naf:  1.0\nn_classes:  2\n","output_type":"stream"}]},{"cell_type":"code","source":"start = 0 if k_start == -1 else k_start\nend = k if k_end == -1 else k_end\n\nall_test_auc = []\nall_val_auc = []\nall_test_acc = []\nall_val_acc = []\nfolds = np.arange(start, end)\nfor i in folds:\n    seed_torch(seed)\n    train_dataset, val_dataset, test_dataset = dataset.return_splits(from_id=False,\n            csv_path='{}/splits_{}.csv'.format(split_dir, i))\n\n    datasets = (train_dataset, val_dataset, test_dataset)\n\n    results, test_auc, val_auc, test_acc, val_acc  = train(datasets, i, settings)\n    all_test_auc.append(test_auc)\n    all_val_auc.append(val_auc)\n    all_test_acc.append(test_acc)\n    all_val_acc.append(val_acc)\n    #write results to pkl\n    filename = os.path.join(results_dir, \"splits_{}\".format(i), 'split_{}_results.pkl'.format(i))\n    save_pkl(filename, results)\n\nfinal_df = pd.DataFrame({'folds': folds, 'test_auc': all_test_auc,\n    'val_auc': all_val_auc, 'test_acc': all_test_acc, 'val_acc' : all_val_acc})\n\nif len(folds) != k:\n    save_name = 'summary_partial_{}_{}.csv'.format(start, end)\nelse:\n    save_name = 'summary.csv'\nfinal_df.to_csv(os.path.join(results_dir, save_name))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:46:45.535666Z","iopub.execute_input":"2022-12-30T10:46:45.536365Z","iopub.status.idle":"2022-12-30T10:46:45.669065Z","shell.execute_reply.started":"2022-12-30T10:46:45.536318Z","shell.execute_reply":"2022-12-30T10:46:45.666493Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"Settings: {'k': 5, 'k_start': -1, 'k_end': -1, 'task': 'task_fungal_vs_nonfungal', 'max_epochs': 200, 'results_dir': '/kaggle/working/results/exp_01_s1', 'lr': 0.0001, 'experiment': 'exp_01', 'reg': 1e-05, 'label_frac': 1.0, 'bag_loss': 'ce', 'seed': 1, 'model_type': 'clam_sb', 'model_size': 'small', 'use_drop_out': False, 'weighted_sample': False, 'opt': 'adam', 'patch_dir': '/kaggle/working/patches', 'feat_dir': '/kaggle/working/features', 'split_dir': '/kaggle/working/splits/fungal_vs_nonfungal_100/', 'log_data': False, 'dataset_csv': '/kaggle/working/fungal_vs_nonfungal.csv', 'testing': False, 'early_stopping': False, 'dropout': True, 'no_inst_cluster': False, 'subtyping': False, 'exp_code': 'exp_01', 'bag_weight': 0.5, 'inst_loss': None, 'B': 12, 'annot_dir': '/kaggle/input/fungal-10x-annot', 'patch_annot_dir': '/kaggle/working/patch_annot', 'alpha_weight': False, 'T1': 50, 'T2': 150, 'af': 1.0, 'n_classes': 2}\n\nTraining Fold 0!\n\nInit train/val/test splits... (<__main__.Generic_Split object at 0x7f68a58019d0>, <__main__.Generic_Split object at 0x7f68b06b74d0>, <__main__.Generic_Split object at 0x7f68a5801050>)\n      case_id  slide_id label  annot\n0      case_0   F005a02     1  False\n1      case_1   F006a01     1  False\n2      case_3   F006a03     1  False\n3      case_4   F006a04     1  False\n4      case_5   F006a05     1  False\n..        ...       ...   ...    ...\n292  case_417  N012a016     0   True\n293  case_418  N012a017     0   True\n294  case_420  N012a019     0   True\n295  case_421  N012a020     0  False\n296  case_422  N017a001     0   True\n\n[297 rows x 4 columns]\n\nDone!\nTraining on 297 samples\nValidating on 297 samples\nTesting on 297 samples\n\nInit loss function... Done!\n\nInit Model... Done!\nCLAM_SB(\n  (attention_net): Sequential(\n    (0): Linear(in_features=1024, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.25, inplace=False)\n    (3): Attn_Net_Gated(\n      (attention_a): Sequential(\n        (0): Linear(in_features=512, out_features=256, bias=True)\n        (1): Tanh()\n        (2): Dropout(p=0.25, inplace=False)\n      )\n      (attention_b): Sequential(\n        (0): Linear(in_features=512, out_features=256, bias=True)\n        (1): Sigmoid()\n        (2): Dropout(p=0.25, inplace=False)\n      )\n      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n  (instance_classifiers): ModuleList(\n    (0): Linear(in_features=512, out_features=2, bias=True)\n    (1): Linear(in_features=512, out_features=2, bias=True)\n  )\n  (instance_loss_fn): CrossEntropyLoss()\n)\nTotal number of parameters: 790791\nTotal number of trainable parameters: 790791\n\nInit optimizer ... Done!\n\nInit Loaders... Done!\n\nSetup EarlyStopping... Done!\n\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/550661889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mall_test_auc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mall_val_auc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/4029997831.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(datasets, cur, settings)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mweight_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_alpha_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'T1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'T2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'af'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'clam_sb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clam_mb'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no_inst_cluster'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mtrain_loop_clam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bag_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             stop = validate_clam(cur, epoch, model, val_loader, settings['n_classes'],\n\u001b[1;32m    210\u001b[0m                 early_stopping, writer, loss_fn, settings['results_dir'])\n","\u001b[0;32m/tmp/ipykernel_27/4029997831.py\u001b[0m in \u001b[0;36mtrain_loop_clam\u001b[0;34m(epoch, model, loader, optimizer, n_classes, bag_weight, writer, loss_fn, alpha_weight, weight_alpha)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_annot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_annot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/93431367.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mbool_annot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslide_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annot'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0mpatch_annot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_annot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslide_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslide_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0mpatch_annot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_annot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mpatch_annot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_annot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bin_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdiscarded\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     ends with a separator.\"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not tuple"],"ename":"TypeError","evalue":"expected str, bytes or os.PathLike object, not tuple","output_type":"error"}]},{"cell_type":"code","source":"!ls features","metadata":{"execution":{"iopub.status.busy":"2023-01-02T05:51:11.906385Z","iopub.execute_input":"2023-01-02T05:51:11.906841Z","iopub.status.idle":"2023-01-02T05:51:13.007808Z","shell.execute_reply.started":"2023-01-02T05:51:11.906740Z","shell.execute_reply":"2023-01-02T05:51:13.006323Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"F005a02.npy  F021a02.npy  F053a06.npy\tN004a043.npy  N004a128.npy\nF006a01.npy  F021a03.npy  F053a07.npy\tN004a044.npy  N004a129.npy\nF006a02.npy  F021a04.npy  F053a08.npy\tN004a045.npy  N004a130.npy\nF006a03.npy  F021a05.npy  F053a09.npy\tN004a046.npy  N004a131.npy\nF006a04.npy  F030a01.npy  F053a10.npy\tN004a047.npy  N004a132.npy\nF006a05.npy  F030a02.npy  F053a11.npy\tN004a048.npy  N004a133.npy\nF006a06.npy  F030a03.npy  F053a12.npy\tN004a049.npy  N004a134.npy\nF006a07.npy  F030a04.npy  F053a13.npy\tN004a050.npy  N004a135.npy\nF006a08.npy  F030a05.npy  F053a14.npy\tN004a051.npy  N004a136.npy\nF006a09.npy  F030a06.npy  F053a15.npy\tN004a052.npy  N004a137.npy\nF006a10.npy  F030a07.npy  F053a16.npy\tN004a053.npy  N005a001.npy\nF007a01.npy  F030a08.npy  F053a17.npy\tN004a054.npy  N005a002.npy\nF007a02.npy  F030a09.npy  F053a18.npy\tN004a055.npy  N005a003.npy\nF007a03.npy  F030a10.npy  F053a19.npy\tN004a056.npy  N005a004.npy\nF007a04.npy  F030a11.npy  F056a01.npy\tN004a057.npy  N005a005.npy\nF007a05.npy  F030a12.npy  F056a02.npy\tN004a058.npy  N005a006.npy\nF007a06.npy  F030a13.npy  F056a03.npy\tN004a059.npy  N005a007.npy\nF007a07.npy  F030a14.npy  F056a04.npy\tN004a060.npy  N005a008.npy\nF007a08.npy  F030a15.npy  F056a05.npy\tN004a061.npy  N005a009.npy\nF007a09.npy  F030a17.npy  F056a06.npy\tN004a062.npy  N005a010.npy\nF007a10.npy  F030a18.npy  F056a07.npy\tN004a063.npy  N005a011.npy\nF007a11.npy  F033a01.npy  F056a08.npy\tN004a064.npy  N005a012.npy\nF007a12.npy  F033a02.npy  F056a09.npy\tN004a065.npy  N005a013.npy\nF007a13.npy  F033a03.npy  F056a10.npy\tN004a066.npy  N005a014.npy\nF007a15.npy  F033a04.npy  F056a11.npy\tN004a067.npy  N005a015.npy\nF007a16.npy  F033a05.npy  F056a12.npy\tN004a068.npy  N005a016.npy\nF007a17.npy  F033a07.npy  F056a13.npy\tN004a069.npy  N005a017.npy\nF007a18.npy  F033a09.npy  F056a14.npy\tN004a070.npy  N005a018.npy\nF007a19.npy  F033a10.npy  F056a15.npy\tN004a071.npy  N005a019.npy\nF007a21.npy  F033a11.npy  F056a16.npy\tN004a072.npy  N005a020.npy\nF007a22.npy  F033a12.npy  F056a17.npy\tN004a073.npy  N005a021.npy\nF009a01.npy  F033a13.npy  F056a18.npy\tN004a074.npy  N005a022.npy\nF009a02.npy  F033a14.npy  F057a01.npy\tN004a075.npy  N005a023.npy\nF009a03.npy  F033a15.npy  F058a01.npy\tN004a076.npy  N006a001.npy\nF009a04.npy  F033a16.npy  F058a02.npy\tN004a077.npy  N006a002.npy\nF010a01.npy  F033a17.npy  F058a03.npy\tN004a078.npy  N006a004.npy\nF010a02.npy  F033a18.npy  F058a04.npy\tN004a079.npy  N006a005.npy\nF010a03.npy  F033a19.npy  F058a05.npy\tN004a080.npy  N006a006.npy\nF010a04.npy  F033a20.npy  N001a001.npy\tN004a081.npy  N006a007.npy\nF010a05.npy  F033a21.npy  N001a002.npy\tN004a082.npy  N006a008.npy\nF011a01.npy  F033a22.npy  N003a001.npy\tN004a083.npy  N006a009.npy\nF011a02.npy  F033a23.npy  N003a002.npy\tN004a084.npy  N006a010.npy\nF012a01.npy  F033a24.npy  N003a003.npy\tN004a085.npy  N007a001.npy\nF012a02.npy  F033a25.npy  N004a001.npy\tN004a086.npy  N007a002.npy\nF012a03.npy  F033a26.npy  N004a002.npy\tN004a087.npy  N007a003.npy\nF012a04.npy  F033a27.npy  N004a003.npy\tN004a088.npy  N007a004.npy\nF012a06.npy  F033a28.npy  N004a004.npy\tN004a089.npy  N009a001.npy\nF013a01.npy  F033a29.npy  N004a005.npy\tN004a090.npy  N009a002.npy\nF013a02.npy  F034a05.npy  N004a006.npy\tN004a091.npy  N009a003.npy\nF013a03.npy  F034a06.npy  N004a007.npy\tN004a092.npy  N009a004.npy\nF013a04.npy  F034a07.npy  N004a008.npy\tN004a093.npy  N009a005.npy\nF013a05.npy  F034a08.npy  N004a009.npy\tN004a094.npy  N009a006.npy\nF013a06.npy  F034a09.npy  N004a010.npy\tN004a095.npy  N011a001.npy\nF013a07.npy  F034a10.npy  N004a011.npy\tN004a096.npy  N011a002.npy\nF013a08.npy  F034a11.npy  N004a012.npy\tN004a097.npy  N011a003.npy\nF013a09.npy  F048a01.npy  N004a013.npy\tN004a098.npy  N011a004.npy\nF013a10.npy  F048a02.npy  N004a014.npy\tN004a099.npy  N011a005.npy\nF013a11.npy  F050a01.npy  N004a015.npy\tN004a100.npy  N011a006.npy\nF013a12.npy  F050a02.npy  N004a016.npy\tN004a101.npy  N011a007.npy\nF013a13.npy  F050a03.npy  N004a017.npy\tN004a102.npy  N011a008.npy\nF013a15.npy  F052a01.npy  N004a018.npy\tN004a103.npy  N011a009.npy\nF015a01.npy  F052a02.npy  N004a019.npy\tN004a104.npy  N011a010.npy\nF017a01.npy  F052a03.npy  N004a020.npy\tN004a105.npy  N012a001.npy\nF017a02.npy  F052a04.npy  N004a021.npy\tN004a106.npy  N012a002.npy\nF017a04.npy  F052a05.npy  N004a022.npy\tN004a107.npy  N012a003.npy\nF017a05.npy  F052a06.npy  N004a023.npy\tN004a108.npy  N012a004.npy\nF017a06.npy  F052a07.npy  N004a024.npy\tN004a109.npy  N012a005.npy\nF017a07.npy  F052a08.npy  N004a025.npy\tN004a110.npy  N012a006.npy\nF017a08.npy  F052a09.npy  N004a026.npy\tN004a111.npy  N012a007.npy\nF017a09.npy  F052a10.npy  N004a027.npy\tN004a112.npy  N012a008.npy\nF017a10.npy  F052a11.npy  N004a028.npy\tN004a113.npy  N012a009.npy\nF018a01.npy  F052a12.npy  N004a029.npy\tN004a114.npy  N012a010.npy\nF018a02.npy  F052a13.npy  N004a030.npy\tN004a115.npy  N012a011.npy\nF018a03.npy  F052a14.npy  N004a031.npy\tN004a116.npy  N012a012.npy\nF018a04.npy  F052a15.npy  N004a032.npy\tN004a117.npy  N012a013.npy\nF018a05.npy  F052a16.npy  N004a033.npy\tN004a118.npy  N012a014.npy\nF018a06.npy  F052a17.npy  N004a034.npy\tN004a119.npy  N012a015.npy\nF018a07.npy  F052a18.npy  N004a035.npy\tN004a120.npy  N012a016.npy\nF018a08.npy  F052a19.npy  N004a036.npy\tN004a121.npy  N012a017.npy\nF018a09.npy  F052a20.npy  N004a037.npy\tN004a122.npy  N012a018.npy\nF018a10.npy  F053a01.npy  N004a038.npy\tN004a123.npy  N012a019.npy\nF018a11.npy  F053a02.npy  N004a039.npy\tN004a124.npy  N012a020.npy\nF018a12.npy  F053a03.npy  N004a040.npy\tN004a125.npy  N017a001.npy\nF018a13.npy  F053a04.npy  N004a041.npy\tN004a126.npy\nF021a01.npy  F053a05.npy  N004a042.npy\tN004a127.npy\n","output_type":"stream"}]}]}